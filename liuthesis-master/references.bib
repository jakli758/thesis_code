
@misc{kazerouni_diffusion_2023,
	title = {Diffusion {Models} for {Medical} {Image} {Analysis}: {A} {Comprehensive} {Survey}},
	shorttitle = {Diffusion {Models} for {Medical} {Image} {Analysis}},
	url = {http://arxiv.org/abs/2211.07804},
	doi = {10.48550/arXiv.2211.07804},
	abstract = {Denoising diffusion models, a class of generative models, have garnered immense interest lately in various deep-learning problems. A diffusion probabilistic model defines a forward diffusion stage where the input data is gradually perturbed over several steps by adding Gaussian noise and then learns to reverse the diffusion process to retrieve the desired noise-free data from noisy data samples. Diffusion models are widely appreciated for their strong mode coverage and quality of the generated samples despite their known computational burdens. Capitalizing on the advances in computer vision, the field of medical imaging has also observed a growing interest in diffusion models. To help the researcher navigate this profusion, this survey intends to provide a comprehensive overview of diffusion models in the discipline of medical image analysis. Specifically, we introduce the solid theoretical foundation and fundamental concepts behind diffusion models and the three generic diffusion modelling frameworks: diffusion probabilistic models, noise-conditioned score networks, and stochastic differential equations. Then, we provide a systematic taxonomy of diffusion models in the medical domain and propose a multi-perspective categorization based on their application, imaging modality, organ of interest, and algorithms. To this end, we cover extensive applications of diffusion models in the medical domain. Furthermore, we emphasize the practical use case of some selected approaches, and then we discuss the limitations of the diffusion models in the medical domain and propose several directions to fulfill the demands of this field. Finally, we gather the overviewed studies with their available open-source implementations at https://github.com/amirhossein-kz/Awesome-Diffusion-Models-in-Medical-Imaging.},
	urldate = {2026-01-20},
	publisher = {arXiv},
	author = {Kazerouni, Amirhossein and Aghdam, Ehsan Khodapanah and Heidari, Moein and Azad, Reza and Fayyaz, Mohsen and Hacihaliloglu, Ilker and Merhof, Dorit},
	month = jun,
	year = {2023},
	note = {arXiv:2211.07804 [eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, diffusion},
	file = {Preprint PDF:C\:\\Users\\jakob\\Zotero\\storage\\N593JRHA\\Kazerouni et al. - 2023 - Diffusion Models for Medical Image Analysis A Comprehensive Survey.pdf:application/pdf;Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\72N57S42\\2211.html:text/html},
}

@misc{zhu_unpaired_2018,
	title = {Unpaired {Image}-to-{Image} {Translation} using {Cycle}-{Consistent} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1703.10593},
	doi = {10.48550/arXiv.1703.10593},
	abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain \$X\$ to a target domain \$Y\$ in the absence of paired examples. Our goal is to learn a mapping \$G: X {\textbackslash}rightarrow Y\$ such that the distribution of images from \$G(X)\$ is indistinguishable from the distribution \$Y\$ using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping \$F: Y {\textbackslash}rightarrow X\$ and introduce a cycle consistency loss to push \$F(G(X)) {\textbackslash}approx X\$ (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
	urldate = {2026-01-20},
	publisher = {arXiv},
	author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv:1703.10593 [cs]
version: 6},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\jakob\\Zotero\\storage\\4NIF76FP\\Zhu et al. - 2018 - Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks.pdf:application/pdf;Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\68RYEA92\\1703.html:text/html},
}

@article{islam_generative_2024,
	title = {Generative {Adversarial} {Networks} ({GANs}) in {Medical} {Imaging}: {Advancements}, {Applications}, and {Challenges}},
	volume = {12},
	issn = {2169-3536},
	shorttitle = {Generative {Adversarial} {Networks} ({GANs}) in {Medical} {Imaging}},
	url = {https://ieeexplore.ieee.org/abstract/document/10445413},
	doi = {10.1109/ACCESS.2024.3370848},
	abstract = {Generative Adversarial Networks are a class of artificial intelligence algorithms that consist of a generator and a discriminator trained simultaneously through adversarial training. GANs have found crucial applications in various fields, including medical imaging. In healthcare, GANs contribute by generating synthetic medical images, enhancing data quality, and aiding in image segmentation, disease detection, and medical image synthesis. Their importance lies in their ability to generate realistic images, facilitating improved diagnostics, research, and training for medical professionals. Understanding its applications, algorithms, current advancements, and challenges is imperative for further advancement in the medical imaging domain. However, no study explores the recent state-of-the-art development of GANs in medical imaging. To overcome this research gap, in this extensive study, we began by exploring the vast array of applications of GANs in medical imaging, scrutinizing them within recent research. We then dive into the prevalent datasets and pre-processing techniques to enhance comprehension. Subsequently, an in-depth discussion of the GAN algorithms, elucidating their respective strengths and limitations, is provided. After that, we meticulously analyzed the results and experimental details of some recent cutting-edge research to obtain a more comprehensive understanding of the current development of GANs in medical imaging. Lastly, we discussed the diverse challenges encountered and future research directions to mitigate these concerns. This systematic review offers a complete overview of GANs in medical imaging, encompassing their application domains, models, state-of-the-art results analysis, challenges, and research directions, serving as a valuable resource for multidisciplinary studies.},
	urldate = {2026-01-20},
	journal = {IEEE Access},
	author = {Islam, Showrov and Aziz, Md. Tarek and Nabil, Hadiur Rahman and Jim, Jamin Rahman and Mridha, M. F. and Kabir, Md. Mohsin and Asai, Nobuyoshi and Shin, Jungpil},
	year = {2024},
	keywords = {Artificial intelligence, Biomedical imaging, Generative adversarial networks, Image reconstruction, Image segmentation, Image synthesis, Magnetic resonance imaging, Medical diagnostic imaging, medical image augmentation, medical image enhancement, medical image segmentation, medical image synthesis, medical imaging, Medical services, Reviews, Systematics, Training},
	pages = {35728--35753},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\8KRJPSM8\\Islam et al. - 2024 - Generative Adversarial Networks (GANs) in Medical Imaging Advancements, Applications, and Challenge.pdf:application/pdf},
}

@article{shi_diffusion_2025,
	title = {Diffusion {Models} for {Medical} {Image} {Computing}: {A} {Survey}},
	volume = {30},
	issn = {1007-0214},
	shorttitle = {Diffusion {Models} for {Medical} {Image} {Computing}},
	url = {https://ieeexplore.ieee.org/abstract/document/10676408},
	doi = {10.26599/TST.2024.9010047},
	abstract = {Diffusion models are a type of generative deep learning model that can process medical images more efficiently than traditional generative models. They have been applied to several medical image computing tasks. This paper aims to help researchers understand the advancements of diffusion models in medical image computing. It begins by describing the fundamental principles, sampling methods, and architecture of diffusion models. Subsequently, it discusses the application of diffusion models in five medical image computing tasks: image generation, modality conversion, image segmentation, image denoising, and anomaly detection. Additionally, this paper conducts fine-tuning of a large model for image generation tasks and comparative experiments between diffusion models and traditional generative models across these five tasks. The evaluation of the fine-tuned large model shows its potential for clinical applications. Comparative experiments demonstrate that diffusion models have a distinct advantage in tasks related to image generation, modality conversion, and image denoising. However, they require further optimization in image segmentation and anomaly detection tasks to match the efficacy of traditional models. Our codes are publicly available at: https://github.com/hiahub/CodeForDiffusion.},
	number = {1},
	urldate = {2026-01-20},
	journal = {Tsinghua Science and Technology},
	author = {Shi, Yaqing and Abulizi, Abudukelimu and Wang, Hao and Feng, Ke and Abudukelimu, Nihemaiti and Su, Youli and Abudukelimu, Halidanmu},
	month = feb,
	year = {2025},
	keywords = {Image segmentation, Image synthesis, Computational modeling, diffusion models, Diffusion models, Face recognition, generative models, large model, medical image, Microwave integrated circuits, Surveys},
	pages = {357--383},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\8CKSZNNH\\Shi et al. - 2025 - Diffusion Models for Medical Image Computing A Survey.pdf:application/pdf},
}

@article{bogl_surveillance_2022,
	title = {Surveillance of atypical femoral fractures in a nationwide fracture register},
	volume = {93},
	issn = {1745-3674},
	url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC8815300/},
	doi = {10.2340/17453674.2022.1380},
	abstract = {Background and purpose
To continuously assess the incidence of atypical femoral fractures (AFFs) in the population is important, to allow the evaluation of the risks and benefits associated with osteoporosis treatment. Therefore, we investigated the possibility to use the Swedish Fracture Register (SFR) as a surveillance tool for AFFs in the population and to explore means of improvement.

Patients and methods
All AFF registrations in the SFR from January 1, 2015 to December 31, 2018 were enrolled in the study. For these patients, radiographs were obtained and combined with radiographs from 176 patients with normal femoral fractures, to form the study cohort. All images were reviewed and classified into AFFs or normal femur fractures by 2 experts in the field (gold-standard classification) and 1 orthopedic resident educated on the specific radiographic features of AFF (educated-user classification). Furthermore, we estimated the incidence rate of AFFs in the population captured by the register through comparison with a previous cohort and calculated the positive predictive value (PPV) and, where possible, the inter-observer agreement (Cohen’s kappa) between the different classifications.

Results
Of the 178 available patients with AFF in the SFR, 104 patients were classified as AFF using the goldstandard classification, and 89 using the educated-user classification. The PPV increased from 0.58 in the SFR classification to 0.93 in the educated-user classification. The interobserver agreement between the gold-standard classification and the educated-user classification was 0.81.

Interpretation
With a positive predictive value of 0.58 the Swedish Fracture Register outperforms radiology reports and reports to the Swedish Medical Products Agency on adverse drug reactions as a diagnostic tool to identify atypical femoral fractures.},
	urldate = {2026-01-20},
	journal = {Acta Orthopaedica},
	author = {BÖGL, Hans Peter and ZDOLSEK, Georg and BARNISIN, Lukas and MÖLLER, Michael and SCHILCHER, Jörg},
	month = jan,
	year = {2022},
	pages = {229--233},
	file = {Volltext:C\:\\Users\\jakob\\Zotero\\storage\\6JWAPIWQ\\BÖGL et al. - 2022 - Surveillance of atypical femoral fractures in a nationwide fracture register.pdf:application/pdf},
}

@article{borji_pros_2019,
	title = {Pros and cons of {GAN} evaluation measures},
	volume = {179},
	issn = {1077-3142},
	url = {https://www.sciencedirect.com/science/article/pii/S1077314218304272},
	doi = {10.1016/j.cviu.2018.10.009},
	abstract = {Generative models, in particular generative adversarial networks (GANs), have gained significant attention in recent years. A number of GAN variants have been proposed and have been utilized in many applications. Despite large strides in terms of theoretical progress, evaluating and comparing GANs remains a daunting task. While several measures have been introduced, as of yet, there is no consensus as to which measure best captures strengths and limitations of models and should be used for fair model comparison. As in other areas of computer vision and machine learning, it is critical to settle on one or few good measures to steer the progress in this field. In this paper, I review and critically discuss more than 24 quantitative and 5 qualitative measures for evaluating generative models with a particular emphasis on GAN-derived models. I also provide a set of 7 desiderata followed by an evaluation of whether a given measure or a family of measures is compatible with them.},
	urldate = {2026-01-20},
	journal = {Computer Vision and Image Understanding},
	author = {Borji, Ali},
	month = feb,
	year = {2019},
	keywords = {Deep learning, Evaluation, Generative adversarial nets, Generative models, Neural networks},
	pages = {41--65},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\JUP97WZJ\\Borji - 2019 - Pros and cons of GAN evaluation measures.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\QNX7HEDX\\S1077314218304272.html:text/html},
}

@article{chen_generative_2022,
	title = {Generative {Adversarial} {Networks} in {Medical} {Image} augmentation: {A} review},
	volume = {144},
	issn = {0010-4825},
	shorttitle = {Generative {Adversarial} {Networks} in {Medical} {Image} augmentation},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482522001743},
	doi = {10.1016/j.compbiomed.2022.105382},
	abstract = {Object
With the development of deep learning, the number of training samples for medical image-based diagnosis and treatment models is increasing. Generative Adversarial Networks (GANs) have attracted attention in medical image processing due to their excellent image generation capabilities and have been widely used in data augmentation. In this paper, a comprehensive and systematic review and analysis of medical image augmentation work are carried out, and its research status and development prospects are reviewed.
Method
This paper reviews 105 medical image augmentation related papers, which mainly collected by ELSEVIER, IEEE Xplore, and Springer from 2018 to 2021. We counted these papers according to the parts of the organs corresponding to the images, and sorted out the medical image datasets that appeared in them, the loss function in model training, and the quantitative evaluation metrics of image augmentation. At the same time, we briefly introduce the literature collected in three journals and three conferences that have received attention in medical image processing.
Result
First, we summarize the advantages of various augmentation models, loss functions, and evaluation metrics. Researchers can use this information as a reference when designing augmentation tasks. Second, we explore the relationship between augmented models and the amount of the training set, and tease out the role that augmented models may play when the quality of the training set is limited. Third, the statistical number of papers shows that the development momentum of this research field remains strong. Furthermore, we discuss the existing limitations of this type of model and suggest possible research directions.
Conclusion
We discuss GAN-based medical image augmentation work in detail. This method effectively alleviates the challenge of limited training samples for medical image diagnosis and treatment models. It is hoped that this review will benefit researchers interested in this field.},
	urldate = {2026-01-20},
	journal = {Computers in Biology and Medicine},
	author = {Chen, Yizhou and Yang, Xu-Hua and Wei, Zihan and Heidari, Ali Asghar and Zheng, Nenggan and Li, Zhicheng and Chen, Huiling and Hu, Haigen and Zhou, Qianwei and Guan, Qiu},
	month = may,
	year = {2022},
	keywords = {Generative adversarial networks, Image synthesis, Deep learning, Augmentation, Medical image},
	pages = {105382},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\YAPX7J9H\\Chen et al. - 2022 - Generative Adversarial Networks in Medical Image augmentation A review.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\ZTFN8BJL\\S0010482522001743.html:text/html},
}

@article{fang_conditional_2024,
	title = {Conditional {Diffusion} {Model} for {X}-{Ray} {Segmentation} {Data} {Generation}},
	volume = {7},
	issn = {23718315},
	url = {https://www.clausiuspress.com/article/10843.html},
	doi = {10.23977/jaip.2024.070102},
	abstract = {Nowadays training a well-functioning deep learning AI model requires a large amount of data, while in the field of medicine many scenarios lack training data due to privacy issues and legal reasons. In this essay, we propose to use ControlNet, a novel approach that leverages stable diffusion models and conditional control to produce realistic and diverse medical images. ControlNet allows us to specify extra conditions that the diffusion model should follow, such as edge maps, depth maps, segmentation masks, or CLIP image embeddings. These conditions can help us to preserve the structure, shape, and semantics of the target organs or tissues, as well as to manipulate the appearance, style, and context of the generated images. Specifically, we will use ControlNet to generate X-ray of a patient with pulmonary nodules and show the improvement.},
	language = {en},
	number = {1},
	urldate = {2026-01-20},
	journal = {Journal of Artificial Intelligence Practice},
	author = {Fang, Zehao},
	year = {2024},
	file = {PDF:C\:\\Users\\jakob\\Zotero\\storage\\UC8AQS4A\\2024 - Conditional Diffusion Model for X-Ray Segmentation Data Generation.pdf:application/pdf},
}

@article{girgis_atypical_2010,
	title = {Atypical {Femoral} {Fractures} and {Bisphosphonate} {Use}},
	volume = {362},
	issn = {0028-4793},
	url = {https://www.nejm.org/doi/full/10.1056/NEJMc0910389},
	doi = {10.1056/NEJMc0910389},
	abstract = {To the Editor: Since 2007, there have been several reports suggesting a potential association between the use of bisphosphonates and the occurrence of subtrochanteric or so-called atypical femoral fracture.1–4 However, a recent registry-based cross-sectional study did not show a greater frequency of such fractures in patients receiving alendronate.5 Thus, the association between atypical femoral fractures and bisphosphonate use remains an open issue. We reviewed 152 femoral fractures (not including hip) that occurred in 152 patients who were admitted to a tertiary center during a 60-month period from June 2003 through May 2008. The mean (±SD) age of the patients . . .},
	number = {19},
	urldate = {2026-01-20},
	journal = {New England Journal of Medicine},
	publisher = {Massachusetts Medical Society},
	author = {Girgis, Christian M. and Sher, Doron and Seibel, Markus J.},
	month = may,
	year = {2010},
	note = {\_eprint: https://www.nejm.org/doi/pdf/10.1056/NEJMc0910389},
	pages = {1848--1849},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\QBK4LJGH\\Girgis et al. - 2010 - Atypical Femoral Fractures and Bisphosphonate Use.pdf:application/pdf},
}

@inproceedings{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
	urldate = {2026-01-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	year = {2020},
	pages = {6840--6851},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\LC3XQB2D\\Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf},
}

@article{kazeminia_gans_2020,
	title = {{GANs} for medical image analysis},
	volume = {109},
	issn = {0933-3657},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365719311510},
	doi = {10.1016/j.artmed.2020.101938},
	abstract = {Generative adversarial networks (GANs) and their extensions have carved open many exciting ways to tackle well known and challenging medical image analysis problems such as medical image de-noising, reconstruction, segmentation, data simulation, detection or classification. Furthermore, their ability to synthesize images at unprecedented levels of realism also gives hope that the chronic scarcity of labeled data in the medical field can be resolved with the help of these generative models. In this review paper, a broad overview of recent literature on GANs for medical applications is given, the shortcomings and opportunities of the proposed methods are thoroughly discussed, and potential future work is elaborated. We review the most relevant papers published until the submission date. For quick access, essential details such as the underlying method, datasets, and performance are tabulated. An interactive visualization that categorizes all papers to keep the review alive is available at http://livingreview.in.tum.de/GANs\_for\_Medical\_Applications/.},
	urldate = {2026-01-20},
	journal = {Artificial Intelligence in Medicine},
	author = {Kazeminia, Salome and Baur, Christoph and Kuijper, Arjan and van Ginneken, Bram and Navab, Nassir and Albarqouni, Shadi and Mukhopadhyay, Anirban},
	month = sep,
	year = {2020},
	keywords = {Generative adversarial networks, Deep learning, Medical imaging, Survey},
	pages = {101938},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\BHDMWGU7\\Kazeminia et al. - 2020 - GANs for medical image analysis.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\URUU6DXC\\S0933365719311510.html:text/html},
}

@article{lindsey_deep_2018,
	title = {Deep neural network improves fracture detection by clinicians},
	volume = {115},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1806905115},
	doi = {10.1073/pnas.1806905115},
	abstract = {Suspected fractures are among the most common reasons for patients to visit emergency departments (EDs), and X-ray imaging is the primary diagnostic tool used by clinicians to assess patients for fractures. Missing a fracture in a radiograph often has severe consequences for patients, resulting in delayed treatment and poor recovery of function. Nevertheless, radiographs in emergency settings are often read out of necessity by emergency medicine clinicians who lack subspecialized expertise in orthopedics, and misdiagnosed fractures account for upward of four of every five reported diagnostic errors in certain EDs. In this work, we developed a deep neural network to detect and localize fractures in radiographs. We trained it to accurately emulate the expertise of 18 senior subspecialized orthopedic surgeons by having them annotate 135,409 radiographs. We then ran a controlled experiment with emergency medicine clinicians to evaluate their ability to detect fractures in wrist radiographs with and without the assistance of the deep learning model. The average clinician’s sensitivity was 80.8\% (95\% CI, 76.7–84.1\%) unaided and 91.5\% (95\% CI, 89.3–92.9\%) aided, and specificity was 87.5\% (95 CI, 85.3–89.5\%) unaided and 93.9\% (95\% CI, 92.9–94.9\%) aided. The average clinician experienced a relative reduction in misinterpretation rate of 47.0\% (95\% CI, 37.4–53.9\%). The significant improvements in diagnostic accuracy that we observed in this study show that deep learning methods are a mechanism by which senior medical specialists can deliver their expertise to generalists on the front lines of medicine, thereby providing substantial improvements to patient care.},
	number = {45},
	urldate = {2026-01-20},
	journal = {Proceedings of the National Academy of Sciences},
	publisher = {Proceedings of the National Academy of Sciences},
	author = {Lindsey, Robert and Daluiski, Aaron and Chopra, Sumit and Lachapelle, Alexander and Mozer, Michael and Sicular, Serge and Hanel, Douglas and Gardner, Michael and Gupta, Anurag and Hotchkiss, Robert and Potter, Hollis},
	month = nov,
	year = {2018},
	pages = {11591--11596},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\G9GI3MF3\\Lindsey et al. - 2018 - Deep neural network improves fracture detection by clinicians.pdf:application/pdf},
}

@article{schilcher_fusion_2024,
	title = {Fusion of electronic health records and radiographic images for a multimodal deep learning prediction model of atypical femur fractures},
	volume = {168},
	issn = {0010-4825},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482523011691},
	doi = {10.1016/j.compbiomed.2023.107704},
	abstract = {Atypical femur fractures (AFF) represent a very rare type of fracture that can be difficult to discriminate radiologically from normal femur fractures (NFF). AFFs are associated with drugs that are administered to prevent osteoporosis-related fragility fractures, which are highly prevalent in the elderly population. Given that these fractures are rare and the radiologic changes are subtle currently only 7\% of AFFs are correctly identified, which hinders adequate treatment for most patients with AFF. Deep learning models could be trained to classify automatically a fracture as AFF or NFF, thereby assisting radiologists in detecting these rare fractures. Historically, for this classification task, only imaging data have been used, using convolutional neural networks (CNN) or vision transformers applied to radiographs. However, to mimic situations in which all available data are used to arrive at a diagnosis, we adopted an approach of deep learning that is based on the integration of image data and tabular data (from electronic health records) for 159 patients with AFF and 914 patients with NFF. We hypothesized that the combinatorial data, compiled from all the radiology departments of 72 hospitals in Sweden and the Swedish National Patient Register, would improve classification accuracy, as compared to using only one modality. At the patient level, the area under the ROC curve (AUC) increased from 0.966 to 0.987 when using the integrated set of imaging data and seven pre-selected variables, as compared to only using imaging data. More importantly, the sensitivity increased from 0.796 to 0.903. We found a greater impact of data fusion when only a randomly selected subset of available images was used to make the image and tabular data more balanced for each patient. The AUC then increased from 0.949 to 0.984, and the sensitivity increased from 0.727 to 0.849. These AUC improvements are not large, mainly because of the already excellent performance of the CNN (AUC of 0.966) when only images are used. However, the improvement is clinically highly relevant considering the importance of accuracy in medical diagnostics. We expect an even greater effect when imaging data from a clinical workflow, comprising a more diverse set of diagnostic images, are used.},
	urldate = {2026-01-20},
	journal = {Computers in Biology and Medicine},
	author = {Schilcher, Jörg and Nilsson, Alva and Andlid, Oliver and Eklund, Anders},
	month = jan,
	year = {2024},
	keywords = {Deep learning, Atypical femoral fractures, Fusion, Multimodal},
	pages = {107704},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\JMCMTJGC\\Schilcher et al. - 2024 - Fusion of electronic health records and radiographic images for a multimodal deep learning predictio.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\NDRXMHIT\\S0010482523011691.html:text/html},
}

@article{usman_akbar_beware_2025,
	title = {Beware of diffusion models for synthesizing medical images—a comparison with {GANs} in terms of memorizing brain {MRI} and chest x-ray images},
	volume = {6},
	issn = {2632-2153},
	url = {https://doi.org/10.1088/2632-2153/ad9a3a},
	doi = {10.1088/2632-2153/ad9a3a},
	abstract = {Diffusion models were initially developed for text-to-image generation and are now being utilized to generate high quality synthetic images. Preceded by generative adversarial networks (GANs), diffusion models have shown impressive results using various evaluation metrics. However, commonly used metrics such as Frechet inception distance and inception score are not suitable for determining whether diffusion models are simply reproducing the training images. Here we train StyleGAN and a diffusion model, using BRATS20, BRATS21 and a chest x-ray (CXR) pneumonia dataset, to synthesize brain MRI and CXR images, and measure the correlation between the synthetic images and all training images. Our results show that diffusion models are more likely to memorize the training images, compared to StyleGAN, especially for small datasets and when using 2D slices from 3D volumes. Researchers should be careful when using diffusion models (and to some extent GANs) for medical imaging, if the final goal is to share the synthetic images.},
	language = {en},
	number = {1},
	urldate = {2026-01-20},
	journal = {Machine Learning: Science and Technology},
	publisher = {IOP Publishing},
	author = {Usman Akbar, Muhammad and Wang, Wuhao and Eklund, Anders},
	month = jan,
	year = {2025},
	pages = {015022},
	file = {IOP Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\FEYCBLJ2\\Usman Akbar et al. - 2025 - Beware of diffusion models for synthesizing medical images—a comparison with GANs in terms of memori.pdf:application/pdf},
}

@article{zdolsek_deep_2021,
	title = {Deep neural networks with promising diagnostic accuracy for the classification of atypical femoral fractures},
	volume = {92},
	issn = {1745-3674},
	url = {https://doi.org/10.1080/17453674.2021.1891512},
	doi = {10.1080/17453674.2021.1891512},
	abstract = {Background and purpose — A correct diagnosis is essential for the appropriate treatment of patients with atypical femoral fractures (AFFs). The diagnostic accuracy of radiographs with standard radiology reports is very poor. We derived a diagnostic algorithm that uses deep neural networks to enable clinicians to discriminate AFFs from normal femur fractures (NFFs) on conventional radiographs. Patients and methods — We entered 433 radiographs from 149 patients with complete AFF and 549 radiographs from 224 patients with NFF into a convolutional neural network (CNN) that acts as a core classifier in an automated pathway and a manual intervention pathway (manual improvement of image orientation). We tested several deep neural network structures (i.e., VGG19, InceptionV3, and ResNet) to identify the network with the highest diagnostic accuracy for distinguishing AFF from NFF. We applied a transfer learning technique and used 5-fold cross-validation and class activation mapping to evaluate the diagnostic accuracy. Results — In the automated pathway, ResNet50 had the highest diagnostic accuracy, with a mean of 91\% (SD 1.3), as compared with 83\% (SD 1.6) for VGG19, and 89\% (SD 2.5) for InceptionV3. The corresponding accuracy levels for the intervention pathway were 94\% (SD 2.0), 92\% (2.7), and 93\% (3.7), respectively. With regards to sensitivity and specificity, ResNet outperformed the other networks with a mean AUC (area under the curve) value of 0.94 (SD 0.01) and surpassed the accuracy of clinical diagnostics. Interpretation — Artificial intelligence systems show excellent diagnostic accuracies for the rare fracture type of AFF in an experimental setting.},
	number = {4},
	urldate = {2026-01-20},
	journal = {Acta Orthopaedica},
	publisher = {Taylor \& Francis},
	author = {Zdolsek, Georg and Chen, Yupei and Bögl, Hans-Peter and Wang, Chunliang and Woisetschläger, Mischa and Schilcher, Jörg},
	month = jul,
	year = {2021},
	note = {\_eprint: https://doi.org/10.1080/17453674.2021.1891512},
	pages = {394--400},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\QFZ9HJ9W\\Zdolsek et al. - 2021 - Deep neural networks with promising diagnostic accuracy for the classification of atypical femoral f.pdf:application/pdf},
}

@inproceedings{saharia_palette_2022,
	address = {New York, NY, USA},
	series = {{SIGGRAPH} '22},
	title = {Palette: {Image}-to-{Image} {Diffusion} {Models}},
	isbn = {978-1-4503-9337-9},
	shorttitle = {Palette},
	url = {https://dl.acm.org/doi/10.1145/3528233.3530757},
	doi = {10.1145/3528233.3530757},
	abstract = {This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io/ for an overview of the results and code.},
	urldate = {2026-01-20},
	booktitle = {{ACM} {SIGGRAPH} 2022 {Conference} {Proceedings}},
	publisher = {Association for Computing Machinery},
	author = {Saharia, Chitwan and Chan, William and Chang, Huiwen and Lee, Chris and Ho, Jonathan and Salimans, Tim and Fleet, David and Norouzi, Mohammad},
	month = jul,
	year = {2022},
	pages = {1--10},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\LP4RU49S\\Saharia et al. - 2022 - Palette Image-to-Image Diffusion Models.pdf:application/pdf},
}

@article{xia_diffusion_2024,
	title = {A {Diffusion} {Model} {Translator} for {Efficient} {Image}-to-{Image} {Translation}},
	volume = {46},
	issn = {1939-3539},
	url = {https://ieeexplore.ieee.org/abstract/document/10614866},
	doi = {10.1109/TPAMI.2024.3435448},
	abstract = {Applying diffusion models to image-to-image translation (I2I) has recently received increasing attention due to its practical applications. Previous attempts inject information from the source image into each denoising step for an iterative refinement, thus resulting in a time-consuming implementation. We propose an efficient method that equips a diffusion model with a lightweight translator, dubbed a Diffusion Model Translator (DMT), to accomplish I2I. Specifically, we first offer theoretical justification that in employing the pioneering DDPM work for the I2I task, it is both feasible and sufficient to transfer the distribution from one domain to another only at some intermediate step. We further observe that the translation performance highly depends on the chosen timestep for domain transfer, and therefore propose a practical strategy to automatically select an appropriate timestep for a given task. We evaluate our approach on a range of I2I applications, including image stylization, image colorization, segmentation to image, and sketch to image, to validate its efficacy and general utility. The comparisons show that our DMT surpasses existing methods in both quality and efficiency. Code is available at https://github.com/THU-LYJ-Lab/dmt.},
	number = {12},
	urldate = {2026-01-20},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Xia, Mengfei and Zhou, Yu and Yi, Ran and Liu, Yong-Jin and Wang, Wenping},
	month = dec,
	year = {2024},
	keywords = {Training, Diffusion models, generative models, Computer science, deep learning, Diffusion processes, image translation, Noise reduction, Task analysis, Trajectory},
	pages = {10272--10283},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\6L9QUVVQ\\Xia et al. - 2024 - A Diffusion Model Translator for Efficient Image-to-Image Translation.pdf:application/pdf},
}

@inproceedings{tumanyan_plug-and-play_2023,
	title = {Plug-and-{Play} {Diffusion} {Features} for {Text}-{Driven} {Image}-to-{Image} {Translation}},
	url = {https://openaccess.thecvf.com/content/CVPR2023/html/Tumanyan_Plug-and-Play_Diffusion_Features_for_Text-Driven_Image-to-Image_Translation_CVPR_2023_paper.html},
	language = {en},
	urldate = {2026-01-20},
	author = {Tumanyan, Narek and Geyer, Michal and Bagon, Shai and Dekel, Tali},
	year = {2023},
	pages = {1921--1930},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\9IEJ33C7\\Tumanyan et al. - 2023 - Plug-and-Play Diffusion Features for Text-Driven Image-to-Image Translation.pdf:application/pdf},
}

@article{croitoru_diffusion_2023,
	title = {Diffusion {Models} in {Vision}: {A} {Survey}},
	volume = {45},
	issn = {1939-3539},
	shorttitle = {Diffusion {Models} in {Vision}},
	url = {https://ieeexplore.ieee.org/abstract/document/10081412},
	doi = {10.1109/TPAMI.2023.3261988},
	abstract = {Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e., low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.},
	number = {9},
	urldate = {2026-01-20},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Croitoru, Florinel-Alin and Hondru, Vlad and Ionescu, Radu Tudor and Shah, Mubarak},
	month = sep,
	year = {2023},
	keywords = {Training, Computational modeling, diffusion models, Noise reduction, Task analysis, Computer vision, Data models, deep generative modeling, Denoising diffusion models, image generation, Mathematical models, noise conditioned score networks, score-based models},
	pages = {10850--10869},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\JBW6EDPV\\Croitoru et al. - 2023 - Diffusion Models in Vision A Survey.pdf:application/pdf},
}

@misc{xing_cross-conditioned_2024,
	title = {Cross-conditioned {Diffusion} {Model} for {Medical} {Image} to {Image} {Translation}},
	url = {http://arxiv.org/abs/2409.08500},
	doi = {10.48550/arXiv.2409.08500},
	abstract = {Multi-modal magnetic resonance imaging (MRI) provides rich, complementary information for analyzing diseases. However, the practical challenges of acquiring multiple MRI modalities, such as cost, scan time, and safety considerations, often result in incomplete datasets. This affects both the quality of diagnosis and the performance of deep learning models trained on such data. Recent advancements in generative adversarial networks (GANs) and denoising diffusion models have shown promise in natural and medical image-to-image translation tasks. However, the complexity of training GANs and the computational expense associated with diffusion models hinder their development and application in this task. To address these issues, we introduce a Cross-conditioned Diffusion Model (CDM) for medical image-to-image translation. The core idea of CDM is to use the distribution of target modalities as guidance to improve synthesis quality while achieving higher generation efficiency compared to conventional diffusion models. First, we propose a Modalityspecific Representation Model (MRM) to model the distribution of target modalities. Then, we design a Modality-decoupled Diffusion Network (MDN) to efficiently and effectively learn the distribution from MRM. Finally, a Cross-conditioned UNet (C-UNet) with a Condition Embedding module is designed to synthesize the target modalities with the source modalities as input and the target distribution for guidance. Extensive experiments conducted on the BraTS2023 and UPenn-GBM benchmark datasets demonstrate the superiority of our method.},
	language = {en},
	urldate = {2026-01-20},
	publisher = {arXiv},
	author = {Xing, Zhaohu and Yang, Sicheng and Chen, Sixiang and Ye, Tian and Yang, Yijun and Qin, Jing and Zhu, Lei},
	month = sep,
	year = {2024},
	note = {arXiv:2409.08500 [eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {PDF:C\:\\Users\\jakob\\Zotero\\storage\\TCCCZAHH\\Xing et al. - 2024 - Cross-conditioned Diffusion Model for Medical Image to Image Translation.pdf:application/pdf},
}

@article{alam_multiscale_2025,
	title = {Multiscale attention generative adversarial networks for lesion synthesis in chest {X}-ray images},
	volume = {15},
	copyright = {2025 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-025-26628-3},
	doi = {10.1038/s41598-025-26628-3},
	abstract = {Recent advancements in deep learning have led to significant improvements in pneumoconiosis diagnosis from chest X-rays (CXR). However, these models typically require large training datasets, which are challenging to collect due to the rarity of the disease and strict data-sharing limitations. In addition, the process of annotating medical images is labor-intensive, requiring highly skilled radiologists, which further limits data availability. To address this data scarcity, we propose a novel approach to generate synthetic pathology in CXR images, thus augmenting existing datasets and improving model training efficiency. While bidirectional generative adversarial networks (GAN), such as CycleGAN, can perform image translation between domains without paired samples, these methods often struggle to maintain structural and pathological consistency, especially with fine details. This study introduces a multiscale attention-based GAN (MSA-GAN) model that enhances CycleGAN with a multiscale attention generator, local-global discriminators and structural similarity (SSIM) loss, ensuring greater fidelity in preserving structural and pathological details during translation. We utilise MSA-GAN-generated synthetic CXR images to train CNN models for pneumoconiosis classification and lung segmentation tasks. Experimental results indicate that CNN trained on MSA-GAN-generated images outperforms existing CNN-based methods, showing improved accuracy and consistency in both qualitative and quantitative assessments across classification and segmentation tasks.},
	language = {en},
	number = {1},
	urldate = {2026-01-20},
	journal = {Scientific Reports},
	publisher = {Nature Publishing Group},
	author = {Alam, Md Shariful and Meijering, Erik and Wang, Dadong and Sowmya, Arcot},
	month = nov,
	year = {2025},
	keywords = {Computational biology and bioinformatics, Mathematics and computing, important},
	pages = {42475},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\WN6377YU\\Alam et al. - 2025 - Multiscale attention generative adversarial networks for lesion synthesis in chest X-ray images.pdf:application/pdf},
}

@article{ibrahim_generative_2025,
	title = {Generative {AI} for synthetic data across multiple medical modalities: {A} systematic review of recent developments and challenges},
	volume = {189},
	issn = {0010-4825},
	shorttitle = {Generative {AI} for synthetic data across multiple medical modalities},
	url = {https://www.sciencedirect.com/science/article/pii/S0010482525001842},
	doi = {10.1016/j.compbiomed.2025.109834},
	abstract = {This paper presents a comprehensive systematic review of generative models (GANs, VAEs, DMs, and LLMs) used to synthesize various medical data types, including imaging (dermoscopic, mammographic, ultrasound, CT, MRI, and X-ray), text, time-series, and tabular data (EHR). Unlike previous narrowly focused reviews, our study encompasses a broad array of medical data modalities and explores various generative models. Our aim is to offer insights into their current and future applications in medical research, particularly in the context of synthesis applications, generation techniques, and evaluation methods, as well as providing a GitHub repository as a dynamic resource for ongoing collaboration and innovation. Our search strategy queries databases such as Scopus, PubMed, and ArXiv, focusing on recent works from January 2021 to November 2023, excluding reviews and perspectives. This period emphasizes recent advancements beyond GANs, which have been extensively covered in previous reviews. The survey also emphasizes the aspect of conditional generation, which is not focused on in similar work. Key contributions include a broad, multi-modality scope that identifies cross-modality insights and opportunities unavailable in single-modality surveys. While core generative techniques are transferable, we find that synthesis methods often lack sufficient integration of patient-specific context, clinical knowledge, and modality-specific requirements tailored to the unique characteristics of medical data. Conditional models leveraging textual conditioning and multimodal synthesis remain underexplored but offer promising directions for innovation. Our findings are structured around three themes: (1) Synthesis applications, highlighting clinically valid synthesis applications and significant gaps in using synthetic data beyond augmentation, such as for validation and evaluation; (2) Generation techniques, identifying gaps in personalization and cross-modality innovation; and (3) Evaluation methods, revealing the absence of standardized benchmarks, the need for large-scale validation, and the importance of privacy-aware, clinically relevant evaluation frameworks. These findings emphasize the need for benchmarking and comparative studies to promote openness and collaboration.},
	urldate = {2026-01-21},
	journal = {Computers in Biology and Medicine},
	author = {Ibrahim, Mahmoud and Khalil, Yasmina Al and Amirrajab, Sina and Sun, Chang and Breeuwer, Marcel and Pluim, Josien and Elen, Bart and Ertaylan, Gökhan and Dumontier, Michel},
	month = may,
	year = {2025},
	keywords = {Generative models, Medical imaging, EHR and physiological signals, Medical data, Medical text, Synthetic data},
	pages = {109834},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\MBUGXX4H\\Ibrahim et al. - 2025 - Generative AI for synthetic data across multiple medical modalities A systematic review of recent d.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\AE3K7TFF\\S0010482525001842.html:text/html},
}

@inproceedings{maguluri_progressive_2025,
	title = {Progressive {GAN} {Framework} for {Realistic} {Chest} {X}-{Ray} {Synthesis} and {Data} {Augmentation}},
	url = {https://ieeexplore.ieee.org/abstract/document/10883508},
	doi = {10.1109/ICMCSI64620.2025.10883508},
	abstract = {The scarcity of large-scale labeled medical imaging datasets presents a significant bottleneck for advancing diag-nostic model performance in radiology. This study introduces a progressive Generative Adversarial Network (GAN) framework for synthesizing high-quality, anatomically consistent chest x-ray images. The proposed model leverages a generator with progressive upsampling and residual connections to preserve fine-grained anatomical details and a PatchGAN-based discriminator to ensure local and global realism. A comprehensive preprocessing pipeline, including contrast enhancement, morphological operations, Fourier analysis, and data augmentation, was employed to prepare high-quality input data. Quantitative evaluation metrics such as Frechet Inception Distance (FID) and Structural Similarity Index (SSIM) demonstrated the model's effectiveness, achieving low FID scores and high SSIM values, indicative of the synthetic images' similarity to real chest X-rays. Qualitative analyses further validated the generated images' clinical relevance and visual fidelity, highlighting their potential for augmenting medical imaging datasets and supporting downstream diagnostic tasks.},
	urldate = {2026-01-21},
	booktitle = {2025 6th {International} {Conference} on {Mobile} {Computing} and {Sustainable} {Informatics} ({ICMCSI})},
	author = {Maguluri, Kiran Kumar and Ganti, Venkata Krishna Azith Teja and Yasmeen, Zakera and Pandugula, Chandrashekar},
	month = jan,
	year = {2025},
	keywords = {Biomedical imaging, Generative adversarial networks, Medical imaging, Synthetic data, Chest X-ray synthesis, Data augmentation, Generative Adversarial Networks, Medical image augmentation, Mobile computing, Morphological operations, PatchGAN, Pipelines, Radiology, Residual networks, Synthetic data generation, Visualization, X-ray imaging},
	pages = {755--760},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\KZJ6V2EK\\Maguluri et al. - 2025 - Progressive GAN Framework for Realistic Chest X-Ray Synthesis and Data Augmentation.pdf:application/pdf},
}

@book{wang_synthetic_2025,
	title = {Synthetic {X}-ray {Images} of {Femur} {Fractures}},
	url = {https://urn.kb.se/resolve?urn=urn:nbn:se:liu:diva-218223},
	abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
	language = {eng},
	urldate = {2026-01-21},
	author = {Wang, Yaning},
	year = {2025},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\BTXGJRNS\\Wang - 2025 - Synthetic X-ray Images of Femur Fractures.pdf:application/pdf},
}

@article{d_n_assessment_2025,
	title = {Assessment of perceived realism in {AI}-generated synthetic spine fracture {CT} images},
	volume = {33},
	issn = {0928-7329},
	url = {https://doi.org/10.1177/09287329241291368},
	doi = {10.1177/09287329241291368},
	abstract = {Background
Deep learning-based decision support systems require synthetic images generated by adversarial networks, which require clinical evaluation to ensure their quality.
Objective
The study evaluates perceived realism of high-dimension synthetic spine fracture CT images generated Progressive Growing Generative Adversarial Networks (PGGANs).
Method: The study used 2820 spine fracture CT images from 456 patients to train an PGGAN model. The model synthesized images up to 512 × 512 pixels, and the realism of the generated images was assessed using Visual Turing Tests and Fracture Identification Test. Three spine surgeons evaluated the images, and clinical evaluation results were statistically analysed.
Result: Spine surgeons have an average prediction accuracy of nearly 50\% during clinical evaluations, indicating difficulty in distinguishing between real and generated images. The accuracy varies for different dimensions, with synthetic images being more realistic, especially in 512 × 512-dimension images. During FIT, among 16 generated images of each fracture type, 13–15 images were correctly identified, indicating images are more realistic and clearly depict fracture lines in 512 × 512 dimensions.
Conclusion
The study reveals that AI-based PGGAN can generate realistic synthetic spine fracture CT images up to 512 × 512 pixels, making them difficult to distinguish from real images, and improving the automatic spine fracture type detection system.},
	language = {EN},
	number = {2},
	urldate = {2026-01-21},
	journal = {Technology and Health Care},
	publisher = {SAGE Publications},
	author = {D N, Sindhura and Pai, Radhika M and Bhat, Shyamasunder N and Pai M M, Manohara},
	month = mar,
	year = {2025},
	pages = {931--944},
}

@article{d_n_vision_2025,
	title = {Vision transformer and deep learning based weighted ensemble model for automated spine fracture type identification with {GAN} generated {CT} images},
	volume = {15},
	copyright = {2025 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-025-98518-7},
	doi = {10.1038/s41598-025-98518-7},
	abstract = {The most common causes of spine fractures, or vertebral column fractures (VCF), are traumas like falls, injuries from sports, or accidents. CT scans are affordable and effective at detecting VCF types in an accurate manner. VCF type identification in cervical, thoracic, and lumbar (C3-L5) regions is limited and sensitive to inter-observer variability. To solve this problem, this work introduces an autonomous approach for identifying VCF type by developing a novel ensemble model of Vision Transformers (ViT) and best-performing deep learning (DL) models. It assists orthopaedicians in easy and early identification of VCF types. The performance of numerous fine-tuned DL architectures, including VGG16, ResNet50, and DenseNet121, was investigated, and an ensemble classification model was developed to identify the best-performing combination of DL models. A ViT model is also trained to identify VCF. Later, the best-performing DL models and ViT were fused by weighted average technique for type identification. To overcome data limitations, an extended Deep Convolutional Generative Adversarial Network (DCGAN) and Progressive Growing Generative Adversarial Network (PGGAN) were developed. The VGG16-ResNet50-ViT ensemble model outperformed all ensemble models and got an accuracy of 89.98\%. Extended DCGAN and PGGAN augmentation increased the accuracy of type identification to 90.28\% and 93.68\%, respectively. This demonstrates efficacy of PGGANs in augmenting VCF images. The study emphasizes the distinctive contributions of the ResNet50, VGG16, and ViT models in feature extraction, generalization, and global shape-based pattern capturing in VCF type identification. CT scans collected from a tertiary care hospital are used to validate these models.},
	language = {en},
	number = {1},
	urldate = {2026-01-21},
	journal = {Scientific Reports},
	publisher = {Nature Publishing Group},
	author = {D. N., Sindhura and Pai, Radhika M. and Bhat, Shyamasunder N. and Pai, Manohara M. M.},
	month = apr,
	year = {2025},
	keywords = {Medical research, Diseases, Engineering},
	pages = {14408},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\J9LNAMXS\\D. N. et al. - 2025 - Vision transformer and deep learning based weighted ensemble model for automated spine fracture type.pdf:application/pdf},
}

@article{ali_generative_2025,
	title = {Generative {Adversarial} {Networks} ({GANs}) for {Medical} {Image} {Processing}: {Recent} {Advancements}},
	volume = {32},
	issn = {1886-1784},
	shorttitle = {Generative {Adversarial} {Networks} ({GANs}) for {Medical} {Image} {Processing}},
	url = {https://doi.org/10.1007/s11831-024-10174-8},
	doi = {10.1007/s11831-024-10174-8},
	abstract = {Generative Adversarial Networks (GANs) constitute an advanced category of deep learning models that have significantly transformed the domain of generative modelling. They demonstrate a profound capability to produce realistic and high-quality synthetic data across various domains. Recently, GANs have also emerged as a powerful and innovative approach in medical image processing. Numerous scholarly investigations consistently highlight the superiority of GAN-based methodologies in this context. The generation of realistic synthetic images aims to advance segmentation precision, augment image quality, and facilitate multimodal analysis. These enhancements significantly bolster the analytical capabilities of medical professionals, leading to more precise diagnostic evaluations and the formulation of personalized treatment plans, thereby contributing to improved patient prognosis. In this work, we rigorously review the latest advancements in the application of Generative Adversarial Networks (GANs) within the domain of medical imaging, encompassing research published between 2018 and 2024. The corpus of literature selected for this review is derived from the most relevant and authoritative databases, including Elsevier, Springer, IEEE Xplore, and Google Scholar, among others. This review rigorously evaluates scholarly publications employing Generative Adversarial Networks (GANs) for the synthesis and generation of medical images, segmentation of medical imaging data, image-to-image translation in medical contexts, and denoising or reconstruction of medical imagery. The findings of this review present a thorough synthesis of contemporary applications of Generative Adversarial Networks (GANs) in the domain of medical imaging. This investigation serves as a prospective reference in the realm of GAN utilization for medical image processing, offering guidance and insights for current and future research endeavors.},
	language = {en},
	number = {2},
	urldate = {2026-01-21},
	journal = {Archives of Computational Methods in Engineering},
	author = {Ali, Mohd and Ali, Mehboob and Hussain, Mubashir and Koundal, Deepika},
	month = mar,
	year = {2025},
	pages = {1185--1198},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\5FTRFSWM\\Ali et al. - 2025 - Generative Adversarial Networks (GANs) for Medical Image Processing Recent Advancements.pdf:application/pdf},
}

@inproceedings{xu_semi-supervised_2019,
	title = {Semi-{Supervised} {Attention}-{Guided} {CycleGAN} for {Data} {Augmentation} on {Medical} {Images}},
	url = {https://ieeexplore.ieee.org/document/8982932},
	doi = {10.1109/BIBM47256.2019.8982932},
	abstract = {Recently, deep learning methods, in particular, convolutional neural networks (CNNs), have made a massive breakthrough in computer vision. And a big amount of annotated data is the essential cornerstone to reach this success. However, in the medical domain, it is usually difficult (and sometimes even impossible) to get sufficient data for some specific learning tasks. Consequently, in this work, a novel data augmentation solution, called semi-supervised attention-guided CycleGAN (SSA-CycleGAN) is proposed to resolve this problem. Specifically, a cycle-consistency GANs-based model is first proposed to generate synthetic tumor (resp., normal) images from normal (resp., tumor) images. Then, a semi-supervised attention module is further proposed to enhance the model's capability in learning the important details of the training images, which in turns help the generated synthetic images become more realistic. To verify its effectiveness, experimental studies are conducted on three medical image datasets with limited amounts of MRI images, and the proposed SSA-CycleGAN is applied to generate synthetic tumor and normal MRI images for data augmentation. Experimental results show that i) SSA-CycleGAN can add (resp., remove) tumor lesions on (resp., from) the original normal (resp., tumor) images and generate very realistic synthetic tumor (resp. normal) images; and ii) in the ResNet18-based MRI image classification tasks based on these datasets, data augmentation using SSA-CycleGAN achieves much better classification performances than the classic data augmentation methods.},
	urldate = {2026-01-21},
	booktitle = {2019 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine} ({BIBM})},
	author = {Xu, Zhenghua and Qi, Chang and Xu, Guizhi},
	month = nov,
	year = {2019},
	keywords = {Biomedical imaging, Image segmentation, Magnetic resonance imaging, Training, Data augmentation, Attention module, Data Augmentation, Generative adversarial networks (GANs), Generators, Image classification, Image resolution, Lesions, Medical image processing, Shape},
	pages = {563--568},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\S2VJ5PVW\\Xu et al. - 2019 - Semi-Supervised Attention-Guided CycleGAN for Data Augmentation on Medical Images.pdf:application/pdf},
}

@inproceedings{wang_automated_2020,
	title = {Automated {Pneumoconiosis} {Detection} on {Chest} {X}-{Rays} {Using} {Cascaded} {Learning} with {Real} and {Synthetic} {Radiographs}},
	url = {https://ieeexplore.ieee.org/document/9363416},
	doi = {10.1109/DICTA51227.2020.9363416},
	abstract = {Pneumoconiosis is an incurable respiratory disease caused by long-term inhalation of respirable dust. Due to small pneumoconiosis incidence and restrictions on sharing of patient data, the number of available pneumoconiosis X-rays is insufficient, which introduces significant challenges for training deep learning models. In this paper, we use both real and synthetic pneumoconiosis radiographs to train a cascaded machine learning framework for the automated detection of pneumoconiosis, including a machine learning based pixel classifier for lung field segmentation, and Cycle-Consistent Adversarial Networks (CycleGAN) for generating abundant lung field images for training, and a Convolutional Neural Network (CNN) based image classier. Experiments are conducted to compare the classification results from several state-of-the-art machine learning models and ours. Our proposed model outperforms the others and achieves an overall classification accuracy of 90.24\%, a specificity of 88.46\% and an excellent sensitivity of 93.33\% for detecting pneumoconiosis.},
	urldate = {2026-01-21},
	booktitle = {2020 {Digital} {Image} {Computing}: {Techniques} and {Applications} ({DICTA})},
	author = {Wang, Dadong and Arzhaeva, Yulia and Devnath, Liton and Qiao, Maoying and Amirgholipour, Saeed and Liao, Qiyu and McBean, Rhiannon and Hillhouse, James and Luo, Suhuai and Meredith, David and Newbigin, Katrina and Yates, Deborah},
	month = nov,
	year = {2020},
	keywords = {Training, Computational modeling, deep learning, black lung, computer-aided diagnosis, Diagnostic radiography, Lung, pneumoconiosis, Pulmonary diseases, Sensitivity, X-rays},
	pages = {1--6},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\DNBZSSPW\\Wang et al. - 2020 - Automated Pneumoconiosis Detection on Chest X-Rays Using Cascaded Learning with Real and Synthetic R.pdf:application/pdf},
}

@misc{welander_generative_2018,
	title = {Generative {Adversarial} {Networks} for {Image}-to-{Image} {Translation} on {Multi}-{Contrast} {MR} {Images} - {A} {Comparison} of {CycleGAN} and {UNIT}},
	url = {http://arxiv.org/abs/1806.07777},
	doi = {10.48550/arXiv.1806.07777},
	abstract = {In medical imaging, a general problem is that it is costly and time consuming to collect high quality data from healthy and diseased subjects. Generative adversarial networks (GANs) is a deep learning method that has been developed for synthesizing data. GANs can thereby be used to generate more realistic training data, to improve classification performance of machine learning algorithms. Another application of GANs is image-to-image translations, e.g. generating magnetic resonance (MR) images from computed tomography (CT) images, which can be used to obtain multimodal datasets from a single modality. Here, we evaluate two unsupervised GAN models (CycleGAN and UNIT) for image-to-image translation of T1- and T2-weighted MR images, by comparing generated synthetic MR images to ground truth images. We also evaluate two supervised models; a modification of CycleGAN and a pure generator model. A small perceptual study was also performed to evaluate how visually realistic the synthesized images are. It is shown that the implemented GAN models can synthesize visually realistic MR images (incorrectly labeled as real by a human). It is also shown that models producing more visually realistic synthetic images not necessarily have better quantitative error measurements, when compared to ground truth data. Code is available at https://github.com/simontomaskarlsson/GAN-MRI},
	urldate = {2026-01-21},
	publisher = {arXiv},
	author = {Welander, Per and Karlsson, Simon and Eklund, Anders},
	month = jun,
	year = {2018},
	note = {arXiv:1806.07777 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\jakob\\Zotero\\storage\\HNFZHPLK\\Welander et al. - 2018 - Generative Adversarial Networks for Image-to-Image Translation on Multi-Contrast MR Images - A Compa.pdf:application/pdf;Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\W8KHQD4L\\1806.html:text/html},
}

@article{webber_diffusion_2024,
	title = {Diffusion models for medical image reconstruction},
	volume = {1},
	issn = {2976-8705},
	url = {https://doi.org/10.1093/bjrai/ubae013},
	doi = {10.1093/bjrai/ubae013},
	abstract = {Better algorithms for medical image reconstruction can improve image quality and enable reductions in acquisition time and radiation dose. A prior understanding of the distribution of plausible images is key to realising these benefits. Recently, research into deep-learning image reconstruction has started to look into using unsupervised diffusion models, trained only on high-quality medical images (ie, without needing paired scanner measurement data), for modelling this prior understanding. Image reconstruction algorithms incorporating unsupervised diffusion models have already attained state-of-the-art accuracy for reconstruction tasks ranging from highly accelerated MRI to ultra-sparse-view CT and low-dose PET. Key advantages of diffusion model approach over previous deep learning approaches for reconstruction include state-of-the-art image distribution modelling, improved robustness to domain shift, and principled quantification of reconstruction uncertainty. If hallucination concerns can be alleviated, their key advantages and impressive performance could mean these algorithms are better suited to clinical use than previous deep-learning approaches. In this review, we provide an accessible introduction to image reconstruction and diffusion models, outline guidance for using diffusion-model-based reconstruction methodology, summarise modality-specific challenges, and identify key research themes. We conclude with a discussion of the opportunities and challenges of using diffusion models for medical image reconstruction.},
	number = {1},
	urldate = {2026-01-21},
	journal = {BJR{\textbar}Artificial Intelligence},
	author = {Webber, George and Reader, Andrew J},
	month = mar,
	year = {2024},
	pages = {ubae013},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\MJIV6QAX\\Webber und Reader - 2024 - Diffusion models for medical image reconstruction.pdf:application/pdf},
}

@article{hung_med-cdiff_2023,
	title = {Med-{cDiff}: {Conditional} {Medical} {Image} {Generation} with {Diffusion} {Models}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2306-5354},
	shorttitle = {Med-{cDiff}},
	url = {https://www.mdpi.com/2306-5354/10/11/1258},
	doi = {10.3390/bioengineering10111258},
	abstract = {Conditional image generation plays a vital role in medical image analysis as it is effective in tasks such as super-resolution, denoising, and inpainting, among others. Diffusion models have been shown to perform at a state-of-the-art level in natural image generation, but they have not been thoroughly studied in medical image generation with specific conditions. Moreover, current medical image generation models have their own problems, limiting their usage in various medical image generation tasks. In this paper, we introduce the use of conditional Denoising Diffusion Probabilistic Models (cDDPMs) for medical image generation, which achieve state-of-the-art performance on several medical image generation tasks.},
	language = {en},
	number = {11},
	urldate = {2026-01-21},
	journal = {Bioengineering},
	publisher = {Multidisciplinary Digital Publishing Institute},
	author = {Hung, Alex Ling Yu and Zhao, Kai and Zheng, Haoxin and Yan, Ran and Raman, Steven S. and Terzopoulos, Demetri and Sung, Kyunghyun},
	month = nov,
	year = {2023},
	keywords = {diffusion models, generative models, image generation, denoising, inpainting, super-resolution},
	pages = {1258},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\7VSKKJ9U\\Hung et al. - 2023 - Med-cDiff Conditional Medical Image Generation with Diffusion Models.pdf:application/pdf},
}

@article{ozbey_unsupervised_2023,
	title = {Unsupervised {Medical} {Image} {Translation} {With} {Adversarial} {Diffusion} {Models}},
	volume = {42},
	issn = {1558-254X},
	url = {https://ieeexplore.ieee.org/abstract/document/10167641},
	doi = {10.1109/TMI.2023.3290149},
	abstract = {Imputation of missing images via source-to-target modality translation can improve diversity in medical imaging protocols. A pervasive approach for synthesizing target images involves one-shot mapping through generative adversarial networks (GAN). Yet, GAN models that implicitly characterize the image distribution can suffer from limited sample fidelity. Here, we propose a novel method based on adversarial diffusion modeling, SynDiff, for improved performance in medical image translation. To capture a direct correlate of the image distribution, SynDiff leverages a conditional diffusion process that progressively maps noise and source images onto the target image. For fast and accurate image sampling during inference, large diffusion steps are taken with adversarial projections in the reverse diffusion direction. To enable training on unpaired datasets, a cycle-consistent architecture is devised with coupled diffusive and non-diffusive modules that bilaterally translate between two modalities. Extensive assessments are reported on the utility of SynDiff against competing GAN and diffusion models in multi-contrast MRI and MRI-CT translation. Our demonstrations indicate that SynDiff offers quantitatively and qualitatively superior performance against competing baselines.},
	number = {12},
	urldate = {2026-01-21},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Özbey, Muzaffer and Dalmaz, Onat and Dar, Salman U. H. and Bedel, Hasan A. and Özturk, Şaban and Güngör, Alper and Çukur, Tolga},
	month = dec,
	year = {2023},
	keywords = {diffusion, Generative adversarial networks, Image synthesis, Training, Computational modeling, Task analysis, Generators, adversarial, Biological system modeling, generative, Medical image translation, synthesis, unpaired, unsupervised},
	pages = {3524--3539},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\SQVQP44S\\Özbey et al. - 2023 - Unsupervised Medical Image Translation With Adversarial Diffusion Models.pdf:application/pdf},
}

@article{pan_2d_2023,
	title = {{2D} medical image synthesis using transformer-based denoising diffusion probabilistic model},
	volume = {68},
	issn = {0031-9155},
	url = {https://doi.org/10.1088/1361-6560/acca5c},
	doi = {10.1088/1361-6560/acca5c},
	abstract = {Objective. Artificial intelligence (AI) methods have gained popularity in medical imaging research. The size and scope of the training image datasets needed for successful AI model deployment does not always have the desired scale. In this paper, we introduce a medical image synthesis framework aimed at addressing the challenge of limited training datasets for AI models. Approach. The proposed 2D image synthesis framework is based on a diffusion model using a Swin-transformer-based network. This model consists of a forward Gaussian noise process and a reverse process using the transformer-based diffusion model for denoising. Training data includes four image datasets: chest x-rays, heart MRI, pelvic CT, and abdomen CT. We evaluated the authenticity, quality, and diversity of the synthetic images using visual Turing assessments conducted by three medical physicists, and four quantitative evaluations: the Inception score (IS), Fréchet Inception Distance score (FID), feature similarity and diversity score (DS, indicating diversity similarity) between the synthetic and true images. To leverage the framework value for training AI models, we conducted COVID-19 classification tasks using real images, synthetic images, and mixtures of both images. Main results. Visual Turing assessments showed an average accuracy of 0.64 (accuracy converging to indicates a better realistic visual appearance of the synthetic images), sensitivity of 0.79, and specificity of 0.50. Average quantitative accuracy obtained from all datasets were IS = 2.28, FID = 37.27, FDS = 0.20, and DS = 0.86. For the COVID-19 classification task, the baseline network obtained an accuracy of 0.88 using a pure real dataset, 0.89 using a pure synthetic dataset, and 0.93 using a dataset mixed of real and synthetic data. Significance. A image synthesis framework was demonstrated for medical image synthesis, which can generate high-quality medical images of different imaging modalities with the purpose of supplementing existing training sets for AI model deployment. This method has potential applications in many data-driven medical imaging research.},
	language = {en},
	number = {10},
	urldate = {2026-01-21},
	journal = {Physics in Medicine \& Biology},
	publisher = {IOP Publishing},
	author = {Pan, Shaoyan and Wang, Tonghe and Qiu, Richard L J and Axente, Marian and Chang, Chih-Wei and Peng, Junbo and Patel, Ashish B and Shelton, Joseph and Patel, Sagar A and Roper, Justin and Yang, Xiaofeng},
	month = may,
	year = {2023},
	pages = {105004},
	file = {IOP Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\FQZJ8HS6\\Pan et al. - 2023 - 2D medical image synthesis using transformer-based denoising diffusion probabilistic model.pdf:application/pdf},
}

@inproceedings{weber_cascaded_2023,
	address = {Cham},
	title = {Cascaded {Latent} {Diffusion} {Models} for {High}-{Resolution} {Chest} {X}-ray {Synthesis}},
	isbn = {978-3-031-33380-4},
	doi = {10.1007/978-3-031-33380-4_14},
	abstract = {While recent advances in large-scale foundational computer vision models show promising results, their application to the medical domain has not yet been explored in detail. In this paper, we progress into the realms of large-scale modeling in medical synthesis by proposing Cheff - a foundational cascaded latent diffusion model, which generates highly-realistic chest radiographs providing state-of-the-art quality on a 1-megapixel scale. We further propose MaCheX, which is a unified interface for public chest datasets and forms the largest open collection of chest X-rays up to date. With Cheff conditioned on radiological reports, we further guide the synthesis process over text prompts and unveil the research area of report-to-chest-X-ray generation.},
	language = {en},
	booktitle = {Advances in {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {Springer Nature Switzerland},
	author = {Weber, Tobias and Ingrisch, Michael and Bischl, Bernd and Rügamer, David},
	editor = {Kashima, Hisashi and Ide, Tsuyoshi and Peng, Wen-Chih},
	year = {2023},
	keywords = {chest radiograph, image synthesis, latent diffusion model},
	pages = {180--191},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\3LNNXHEA\\Weber et al. - 2023 - Cascaded Latent Diffusion Models for High-Resolution Chest X-ray Synthesis.pdf:application/pdf},
}

@article{hosseini_self-supervised_2025,
	title = {Self-{Supervised} {Learning} {Powered} by {Synthetic} {Data} {From} {Diffusion} {Models}: {Application} to {X}-{Ray} {Images}},
	volume = {13},
	issn = {2169-3536},
	shorttitle = {Self-{Supervised} {Learning} {Powered} by {Synthetic} {Data} {From} {Diffusion} {Models}},
	url = {https://ieeexplore.ieee.org/abstract/document/10945534},
	doi = {10.1109/ACCESS.2025.3555619},
	abstract = {Synthetic data offers a compelling solution to the challenges associated with acquiring high-quality medical data, which is often constrained by privacy concerns and limited accessibility. This study explores the efficacy of synthetic data generated using diffusion models for training deep learning models within a self-supervised learning framework. The primary objective is to evaluate whether synthetic data can effectively preserve critical medical biomarkers and support reliable downstream tasks such as classification and segmentation. Using chest X-ray images as a case study, the results reveal that models pretrained on synthetic data achieve performance comparable to or surpassing those pretrained on real data. Specifically, in pneumonia classification task, the model trained on synthetic data outperformed established benchmarks, achieving an Area Under the Curve of 99.1 and an F1-score of 96.1\%. Similarly, for segmentation tasks, the model trained on synthetic data demonstrated robust performance, attaining a Dice score of 0.85. These findings underscore a significant advancement in the generation of synthetic medical images, providing a viable approach to creating realistic, biomarker-preserving datasets that ensure patient confidentiality and enable diverse applications in medical imaging.},
	urldate = {2026-01-21},
	journal = {IEEE Access},
	author = {Hosseini, Abdullah and Serag, Ahmed},
	year = {2025},
	keywords = {Artificial intelligence, Image segmentation, Medical diagnostic imaging, Training, Diffusion models, deep learning, Diffusion processes, Data models, Synthetic data, X-ray imaging, Biological system modeling, Biomarkers, biomedical imaging, diffusion probabilistic, self-supervised learning, synthetic data},
	pages = {59074--59084},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\DK2W9LRH\\Hosseini und Serag - 2025 - Self-Supervised Learning Powered by Synthetic Data From Diffusion Models Application to X-Ray Image.pdf:application/pdf},
}

@article{moris_adapted_2024,
	title = {Adapted generative latent diffusion models for accurate pathological analysis in chest {X}-ray images},
	volume = {62},
	issn = {1741-0444},
	url = {https://doi.org/10.1007/s11517-024-03056-5},
	doi = {10.1007/s11517-024-03056-5},
	abstract = {Respiratory diseases have a significant global impact, and assessing these conditions is crucial for improving patient outcomes. Chest X-ray is widely used for diagnosis, but expert evaluation can be challenging. Automatic computer-aided diagnosis methods can provide support for clinicians in these tasks. Deep learning has emerged as a set of algorithms with exceptional potential in such tasks. However, these algorithms require a vast amount of data, often scarce in medical imaging domains. In this work, a new data augmentation methodology based on adapted generative latent diffusion models is proposed to improve the performance of an automatic pathological screening in two high-impact scenarios: tuberculosis and lung nodules. The methodology is evaluated using three publicly available datasets, representative of real-world settings. An ablation study obtained the highest-performing image generation model configuration regarding the number of training steps. The results demonstrate that the novel set of generated images can improve the performance of the screening of these two highly relevant pathologies, obtaining an accuracy of 97.09\%, 92.14\% in each dataset of tuberculosis screening, respectively, and 82.19\% in lung nodules. The proposal notably improves on previous image generation methods for data augmentation, highlighting the importance of the contribution in these critical public health challenges.},
	language = {en},
	number = {7},
	urldate = {2026-01-21},
	journal = {Medical \& Biological Engineering \& Computing},
	author = {Morís, Daniel I. and Moura, Joaquim de and Novo, Jorge and Ortega, Marcos},
	month = jul,
	year = {2024},
	keywords = {Deep learning, Chest X-ray, Lung nodules, Stable diffusion, Tuberculosis},
	pages = {2189--2212},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\WEY7NWJ4\\Morís et al. - 2024 - Adapted generative latent diffusion models for accurate pathological analysis in chest X-ray images.pdf:application/pdf},
}

@article{chedid_synthesis_2020,
	title = {Synthesis of fracture radiographs with deep neural networks},
	volume = {8},
	issn = {2047-2501},
	url = {https://doi.org/10.1007/s13755-020-00111-x},
	doi = {10.1007/s13755-020-00111-x},
	abstract = {We describe a machine learning system for converting diagrams of fractures into realistic X-ray images. We further present a method for iterative, human-guided refinement of the generated images and show that the resulting synthetic images can be used during training to increase the accuracy of deep classifiers on clinically meaningful subsets of fracture X-rays.},
	language = {en},
	number = {1},
	urldate = {2026-01-22},
	journal = {Health Information Science and Systems},
	author = {Chedid, Nicholas and Sadda, Praneeth and Gonchigar, Anish and Langdon, Jonathan and Porrino, Jack and Haims, Andrew and Taylor, Richard Andrew},
	month = may,
	year = {2020},
	keywords = {Image synthesis, Deep learning, X-ray},
	pages = {21},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\ZN6NRTUH\\Chedid et al. - 2020 - Synthesis of fracture radiographs with deep neural networks.pdf:application/pdf},
}

@inproceedings{saad_self-attention_2023,
	address = {Cham},
	title = {A {Self}-attention {Guided} {Multi}-scale {Gradient} {GAN} for {Diversified} {X}-ray {Image} {Synthesis}},
	isbn = {978-3-031-26438-2},
	doi = {10.1007/978-3-031-26438-2_2},
	abstract = {Imbalanced image datasets are commonly available in the domain of biomedical image analysis. Biomedical images contain diversified features that are significant in predicting targeted diseases. Generative Adversarial Networks (GANs) are utilized to address the data limitation problem via the generation of synthetic images. Training challenges such as mode collapse, non-convergence, and instability degrade a GAN’s performance in synthesizing diversified and high-quality images. In this work, MSG-SAGAN, an attention-guided multi-scale gradient GAN architecture is proposed to model the relationship between long-range dependencies of biomedical image features and improves the training performance using a flow of multi-scale gradients at multiple resolutions in the layers of generator and discriminator models. The intent is to reduce the impact of mode collapse and stabilize the training of GAN using an attention mechanism with multi-scale gradient learning for diversified X-ray image synthesis. Multi-scale Structural Similarity Index Measure (MS-SSIM) and Frechet Inception Distance (FID) are used to identify the occurrence of mode collapse and evaluate the diversity of synthetic images generated. The proposed architecture is compared with the multi-scale gradient GAN (MSG-GAN) to assess the diversity of generated synthetic images. Results indicate that the MSG-SAGAN outperforms MSG-GAN in synthesizing diversified images as evidenced by the MS-SSIM and FID scores.},
	language = {en},
	booktitle = {Artificial {Intelligence} and {Cognitive} {Science}},
	publisher = {Springer Nature Switzerland},
	author = {Saad, Muhammad Muneeb and Rehmani, Mubashir Husain and O’Reilly, Ruairi},
	editor = {Longo, Luca and O’Reilly, Ruairi},
	year = {2023},
	keywords = {Diversity, FID, GANs, Mode collapse, MS-SSIM, Multi-scale gradients, Self-attention, Synthesis, X-ray images},
	pages = {18--31},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\C47F4HTN\\Saad et al. - 2023 - A Self-attention Guided Multi-scale Gradient GAN for Diversified X-ray Image Synthesis.pdf:application/pdf},
}

@misc{xia_diffi2i_2023,
	title = {{DiffI2I}: {Efficient} {Diffusion} {Model} for {Image}-to-{Image} {Translation}},
	shorttitle = {{DiffI2I}},
	url = {http://arxiv.org/abs/2308.13767},
	doi = {10.48550/arXiv.2308.13767},
	abstract = {The Diffusion Model (DM) has emerged as the SOTA approach for image synthesis. However, the existing DM cannot perform well on some image-to-image translation (I2I) tasks. Different from image synthesis, some I2I tasks, such as super-resolution, require generating results in accordance with GT images. Traditional DMs for image synthesis require extensive iterations and large denoising models to estimate entire images, which gives their strong generative ability but also leads to artifacts and inefficiency for I2I. To tackle this challenge, we propose a simple, efficient, and powerful DM framework for I2I, called DiffI2I. Specifically, DiffI2I comprises three key components: a compact I2I prior extraction network (CPEN), a dynamic I2I transformer (DI2Iformer), and a denoising network. We train DiffI2I in two stages: pretraining and DM training. For pretraining, GT and input images are fed into CPEN\$\_\{S1\}\$ to capture a compact I2I prior representation (IPR) guiding DI2Iformer. In the second stage, the DM is trained to only use the input images to estimate the same IRP as CPEN\$\_\{S1\}\$. Compared to traditional DMs, the compact IPR enables DiffI2I to obtain more accurate outcomes and employ a lighter denoising network and fewer iterations. Through extensive experiments on various I2I tasks, we demonstrate that DiffI2I achieves SOTA performance while significantly reducing computational burdens.},
	urldate = {2026-01-25},
	publisher = {arXiv},
	author = {Xia, Bin and Zhang, Yulun and Wang, Shiyin and Wang, Yitong and Wu, Xinglong and Tian, Yapeng and Yang, Wenming and Timotfe, Radu and Gool, Luc Van},
	month = aug,
	year = {2023},
	note = {arXiv:2308.13767 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\jakob\\Zotero\\storage\\UGF9B3T3\\Xia et al. - 2023 - DiffI2I Efficient Diffusion Model for Image-to-Image Translation.pdf:application/pdf;Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\UKMLZCBF\\2308.html:text/html},
}

@article{li_u-kan_2025,
	title = {U-{KAN} {Makes} {Strong} {Backbone} for {Medical} {Image} {Segmentation} and {Generation}},
	volume = {39},
	copyright = {Copyright (c) 2025 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/32491},
	doi = {10.1609/aaai.v39i5.32491},
	abstract = {U-Net has become a cornerstone in various visual applications such as image segmentation and diffusion probability models. While numerous innovative designs and improvements have been introduced by incorporating transformers or MLPs, the networks are still limited to linearly modeling patterns as well as the deficient interpretability. To address these challenges, our intuition is inspired by the impressive results of the Kolmogorov-Arnold Networks (KANs) in terms of accuracy and interpretability, which reshape the neural network learning via the stack of non-linear learnable activation functions derived from the Kolmogorov-Anold representation theorem. Specifically, in this paper, we explore the untapped potential of KANs in improving backbones for vision tasks. We investigate, modify and re-design the established U-Net pipeline by integrating the dedicated KAN layers on the tokenized intermediate representation, termed U-KAN. Rigorous medical image segmentation benchmarks verify the superiority of UKAN by higher accuracy even with less computation cost. We further delved into the potential of U-KAN as an alternative U-Net noise predictor in diffusion models, demonstrating its applicability in generating task-oriented model architectures.},
	language = {en},
	number = {5},
	urldate = {2026-01-25},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Li, Chenxin and Liu, Xinyu and Li, Wuyang and Wang, Cheng and Liu, Hengyu and Liu, Yifan and Chen, Zhen and Yuan, Yixuan},
	month = apr,
	year = {2025},
	pages = {4652--4660},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\ERFKJJVB\\Li et al. - 2025 - U-KAN Makes Strong Backbone for Medical Image Segmentation and Generation.pdf:application/pdf},
}

@misc{torbunov_uvcgan_2023,
	title = {{UVCGAN} v2: {An} {Improved} {Cycle}-{Consistent} {GAN} for {Unpaired} {Image}-to-{Image} {Translation}},
	shorttitle = {{UVCGAN} v2},
	url = {http://arxiv.org/abs/2303.16280},
	doi = {10.48550/arXiv.2303.16280},
	abstract = {An unpaired image-to-image (I2I) translation technique seeks to find a mapping between two domains of data in a fully unsupervised manner. While initial solutions to the I2I problem were provided by generative adversarial neural networks (GANs), diffusion models (DMs) currently hold the state-of-the-art status on the I2I translation benchmarks in terms of Frechet inception distance (FID). Yet, DMs suffer from limitations, such as not using data from the source domain during the training or maintaining consistency of the source and translated images only via simple pixel-wise errors. This work improves a recent UVCGAN model and equips it with modern advancements in model architectures and training procedures. The resulting revised model significantly outperforms other advanced GAN- and DM-based competitors on a variety of benchmarks. In the case of Male-to-Female translation of CelebA, the model achieves more than 40\% improvement in FID score compared to the state-of-the-art results. This work also demonstrates the ineffectiveness of the pixel-wise I2I translation faithfulness metrics and suggests their revision. The code and trained models are available at https://github.com/LS4GAN/uvcgan2},
	urldate = {2026-01-25},
	publisher = {arXiv},
	author = {Torbunov, Dmitrii and Huang, Yi and Tseng, Huan-Hsin and Yu, Haiwang and Huang, Jin and Yoo, Shinjae and Lin, Meifeng and Viren, Brett and Ren, Yihui},
	month = sep,
	year = {2023},
	note = {arXiv:2303.16280 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\jakob\\Zotero\\storage\\D2RR4C3E\\Torbunov et al. - 2023 - UVCGAN v2 An Improved Cycle-Consistent GAN for Unpaired Image-to-Image Translation.pdf:application/pdf;Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\JCDUSBST\\2303.html:text/html},
}

@article{kausar_sd-gan_2023,
	title = {{SD}-{GAN}: {A} {Style} {Distribution} {Transfer} {Generative} {Adversarial} {Network} for {Covid}-19 {Detection} {Through} {X}-{Ray} {Images}},
	volume = {11},
	issn = {2169-3536},
	shorttitle = {{SD}-{GAN}},
	url = {https://ieeexplore.ieee.org/document/10061161},
	doi = {10.1109/ACCESS.2023.3253282},
	abstract = {The Covid-19 pandemic is a prevalent health concern around the world in recent times. Therefore, it is essential to screen the infected patients at the primary stage to prevent secondary infections from person to person. The reverse transcription polymerase chain reaction (RT-PCR) test is commonly performed for Covid-19 diagnosis, while it requires significant effort from health professionals. Automated Covid-19 diagnosis using chest X-ray images is one of the promising directions to screen infected patients quickly and effectively. Automatic diagnostic approaches are used with the assumption that data originating from different sources have the same feature distributions. However, the X-ray images generated in different laboratories using different devices experience style variations e.g., intensity and contrast which contradict the above assumption. The prediction performance of deep models trained on such heterogeneous images of different distributions with different noises is affected. To address this issue, we have designed an automatic end-to-end adaptive normalization-based model called style distribution transfer generative adversarial network (SD-GAN). The designed model is equipped with the generative adversarial network (GAN) and task-specific classifier to transform the style distribution of images between different datasets belonging to different race people and carried out Covid-19 detection effectively. Evaluated results on four different X-ray datasets show the superiority of the proposed model to state-of-the-art methods in terms of the visual quality of style transferred images and the accuracy of Covid-19 infected patient detection. SD-GAN is publicly available at: https://github.com/tasleem-hello/SD-GAN/tree/SD-GAN.},
	urldate = {2026-01-26},
	journal = {IEEE Access},
	author = {Kausar, Tasleem and Lu, Yun and Kausar, Adeeba and Ali, Mustajab and Yousaf, Adnan},
	year = {2023},
	keywords = {Biomedical imaging, Generative adversarial networks, Task analysis, X-ray imaging, Chest X-ray, Adaptation models, covid-19, COVID-19, generative adversarial learning, style transfer},
	pages = {24545--24560},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\GHPCY9Q7\\Kausar et al. - 2023 - SD-GAN A Style Distribution Transfer Generative Adversarial Network for Covid-19 Detection Through.pdf:application/pdf},
}

@article{chen_compressed_2017,
	title = {Compressed {Medical} {Image} {Quality} {Determination} {Using} the {Kolmogorov}- {Smirnov} {Test}},
	volume = {13},
	abstract = {Background: De-noising is the main effect produced by image compression at low compression ratios, which alters only the noise parts of images. The difference between the original and manipulated images will be less than the statistical variation when pixel values are sampled from these
two images. This study develops a method that determines the quality indication in images using compression. Methods: We use the Kolmogorov-Smirnov (KS) two-sample test to determine if the pixel distribution in compressed images is statistically different from the original and then compare
previous reports to find the conceived image compression level. Medical images are first compressed using JPEG2000 at various degrees. The KS test was then used to find whether the two datasets differ significantly. Four different window sizes with ten to one hundred thousand test positions
are sampled independently from both the compressed and original images to determine their respective empirical distribution functions. Results: The rates for D over the critical value increased obviously with the increasing of image compression ratio. The rates of D values exceeding the
critical values are independent of the numbers of test positions. The conceivable image compression ratio level in this work may be set at 10\% below the rejection rate compared with previous reports. Conclusion: The results of this report prove that the KS test can be used to indicate
the variation in image quality and results were proven equivalent to PSNR. The potential applications for this method include determining the optimal compression ratio for tele-radiology or image archiving.},
	number = {2},
	journal = {Current Medical Imaging},
	author = {Chen, Tzong-Jer and Chuang, Keh-Shih and Wu, Wei and Lu, Yue-Ran},
	month = may,
	year = {2017},
	keywords = {Image compression, JPEG 2000, Kolmogorov-Smirnov test, PSNR, statistically lossless, stats},
	pages = {204--209},
	file = {PDF:C\:\\Users\\jakob\\Zotero\\storage\\DWSVI7Z9\\Chen et al. - 2017 - Compressed Medical Image Quality Determination Using the Kolmogorov- Smirnov Test.pdf:application/pdf},
}

@inproceedings{hossain_no-reference_2024,
	title = {No-{Reference} {Blurred} {Image} {Detection} from {Colonoscopy} {Videos} {Using} {Walsh}-{Hadamard} {Transform} and {Kolmogorov} {Smirnov} {Test}},
	issn = {2832-4234},
	url = {https://ieeexplore.ieee.org/abstract/document/10759253},
	doi = {10.1109/IST63414.2024.10759253},
	abstract = {Endoscopy is a widely used clinical procedure for the early detection of polyps which may develop into cancers if not treated. Colorectal cancer (CRC) being the third most diagnosed cancer in the world requires colonoscopy for diagnosis. Colonoscopy videos often contain blurry frames due to motion, out -of- focus, water jets, etc. making them prone to miss diagnosis due to information loss in the noisy frames. Image Restoration (IR) techniques such as Gaussian and diffusion-based generative denoisers have become very popular recently and have great prospects for restoring medical images. This paper proposes a simple yet effective training-free no-reference colonoscopy image blur detection technique, that can be used in future research for image denoising. This method applies artificial Gaussian blur to the input image and transforms both input and artificial blurred images from the spatial domain to the frequency domain using Walsh-Hadamard (WH) transform. Kolmogorov Smirnov test (KS-statistic) was used to calculate the difference between two distributions of frequencies assuming a blurred image will have a lower difference than a sharp image. Being training-free and using only a single frequency feature this algorithm can be implemented for any domain of images for detecting blurred images, tested on public Image Quality Assessment (IQA) datasets such as CSIQ and LIVE\_II and achieved comparable results. Code and dataset will be available at https://github.com/shakhaout/colorectal-blur-detection.git},
	urldate = {2026-01-27},
	booktitle = {2024 {IEEE} {International} {Conference} on {Imaging} {Systems} and {Techniques} ({IST})},
	author = {Hossain, MD Shakhaout and Ono, Naoaki and Kanaya, Shigehiko and Altaf-Ul-Amin, Md.},
	month = oct,
	year = {2024},
	note = {ISSN: 2832-4234},
	keywords = {Training, medical image, stats, Colonoscopy, Colonoscopy blur image detection, Colorectal cancer, Estimation, frequency domain, Frequency-domain analysis, Image restoration, kolmogorov smirnov test, no reference blur assessment, Real-time systems, Reflection, Transforms, Videos, walsh-hadamard},
	pages = {1--6},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\4LZ8KXLR\\Hossain et al. - 2024 - No-Reference Blurred Image Detection from Colonoscopy Videos Using Walsh-Hadamard Transform and Kolm.pdf:application/pdf},
}

@inproceedings{demidenko_kolmogorov-smirnov_2004,
	address = {Berlin, Heidelberg},
	title = {Kolmogorov-{Smirnov} {Test} for {Image} {Comparison}},
	isbn = {978-3-540-24768-5},
	doi = {10.1007/978-3-540-24768-5_100},
	abstract = {We apply the Kolmogorov-Smirnov test to test whether two distributions of 256 gray intensities are the same. Thus, this test may be useful to compare unstructured images, such as microscopic images in medicine. Usually, histogram is used to show the distribution of gray level intensities. We argue that cumulative distribution function (gray distribution) may be more informative when comparing several gray images. The Kolmogorov-Smirnov test is illustrated by hystology images from untreated and treated breast cancer tumors. The test is generalized to ensembles of gray images. Limitations of the Kolmogorov-Smirnov test are discussed.},
	language = {en},
	booktitle = {Computational {Science} and {Its} {Applications} – {ICCSA} 2004},
	publisher = {Springer},
	author = {Demidenko, Eugene},
	editor = {Laganá, Antonio and Gavrilova, Marina L. and Kumar, Vipin and Mun, Youngsong and Tan, C. J. Kenneth and Gervasi, Osvaldo},
	year = {2004},
	pages = {933--939},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\D7PK9GIK\\Demidenko - 2004 - Kolmogorov-Smirnov Test for Image Comparison.pdf:application/pdf},
}

@article{goodfellow_generative_2020,
	title = {Generative adversarial networks},
	volume = {63},
	issn = {0001-0782},
	url = {https://dl.acm.org/doi/10.1145/3422622},
	doi = {10.1145/3422622},
	abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
	number = {11},
	urldate = {2026-01-28},
	journal = {Commun. ACM},
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = oct,
	year = {2020},
	pages = {139--144},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\TWS2GXQQ\\Goodfellow et al. - 2020 - Generative adversarial networks.pdf:application/pdf},
}

@inproceedings{choi_stargan_2018,
	title = {{StarGAN}: {Unified} {Generative} {Adversarial} {Networks} for {Multi}-{Domain} {Image}-to-{Image} {Translation}},
	shorttitle = {{StarGAN}},
	url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Choi_StarGAN_Unified_Generative_CVPR_2018_paper.html},
	urldate = {2026-01-30},
	author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
	year = {2018},
	pages = {8789--8797},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\M9Z7ZFAI\\Choi et al. - 2018 - StarGAN Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation.pdf:application/pdf},
}

@inproceedings{choi_stargan_2020,
	title = {{StarGAN} v2: {Diverse} {Image} {Synthesis} for {Multiple} {Domains}},
	shorttitle = {{StarGAN} v2},
	url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Choi_StarGAN_v2_Diverse_Image_Synthesis_for_Multiple_Domains_CVPR_2020_paper.html},
	urldate = {2026-01-30},
	author = {Choi, Yunjey and Uh, Youngjung and Yoo, Jaejun and Ha, Jung-Woo},
	year = {2020},
	pages = {8188--8197},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\B2DA7CL8\\Choi et al. - 2020 - StarGAN v2 Diverse Image Synthesis for Multiple Domains.pdf:application/pdf},
}

@inproceedings{peng_image--image_2023,
	title = {Image-to-{Image} {Translation} for {Data} {Augmentation} on {Multimodal}...},
	url = {https://openreview.net/forum?id=fOzgoE792l},
	abstract = {Medical images play an important role in medical diagnosis. However, acquiring a large number of datasets with annotations is still a difficult task in the medical field. For this reason, research...},
	language = {en},
	urldate = {2026-01-30},
	author = {Peng, Yue and Meng, Zuqiang and Yang, Lina},
	year = {2023},
	file = {PDF:C\:\\Users\\jakob\\Zotero\\storage\\3ET7I38W\\Image-to-Image Translation for Data Augmentation on Multimodal....pdf:application/pdf;Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\YHEMT3BP\\forum.html:text/html},
}

@article{chen_multi-domain_2023,
	title = {Multi-domain medical image translation generation for lung image classification based on generative adversarial networks},
	volume = {229},
	issn = {0169-2607},
	url = {https://www.sciencedirect.com/science/article/pii/S0169260722005818},
	doi = {10.1016/j.cmpb.2022.107200},
	abstract = {Objective
Lung image classification-assisted diagnosis has a large application market. Aiming at the problems of poor attention to existing translation models, the insufficient ability of key transfer and generation, insufficient quality of generated images, and lack of detailed features, this paper conducts research on lung medical image translation and lung image classification based on generative adversarial networks.
Methods
This paper proposes a medical image multi-domain translation algorithm MI-GAN based on the key migration branch. After the actual analysis of the imbalanced medical image data, the key target domain images are selected, the key migration branch is established, and a single generator is used to complete the medical image multi-domain translation. The conversion between domains ensures the attention performance of the medical image multi-domain translation model and the quality of the synthesized images. At the same time, a lung image classification model based on synthetic image data augmentation is proposed. The synthetic lung CT medical images and the original real medical images are used as the training set together to study the performance of the auxiliary diagnosis model in the classification of normal healthy subjects, and also of the mild and severe COVID-19 patients.
Results
Based on the chest CT image dataset, MI-GAN has completed the mutual conversion and generation of normal lung images without disease, viral pneumonia and Mild COVID-19 images. The synthetic images GAN-test and GAN-train indicators reached, respectively 92.188\% and 85.069\%, compared with other generative models in terms of authenticity and diversity, there is a considerable improvement. The accuracy rate of pneumonia diagnosis of the lung image classification model is 93.85\%, which is 3.1\% higher than that of the diagnosis model trained only with real images; the sensitivity of disease diagnosis is 96.69\%, a relative improvement of 7.1\%. 1\%, the specificity was 89.70\%; the area under the ROC curve (AUC) increased from 94.00\% to 96.17\%.
Conclusion
In this paper, a multi-domain translation model of medical images based on the key transfer branch is proposed, which enables the translation network to have key transfer and attention performance. It is verified on lung CT images and achieved good results. The required medical images are synthesized by the above medical image translation model, and the effectiveness of the synthesized images on the lung image classification network is verified experimentally.},
	urldate = {2026-01-30},
	journal = {Computer Methods and Programs in Biomedicine},
	author = {Chen, Yunfeng and Lin, Yalan and Xu, Xiaodie and Ding, Jinzhen and Li, Chuzhao and Zeng, Yiming and Xie, Weifang and Huang, Jianlong},
	month = feb,
	year = {2023},
	keywords = {Image processing, Key transfer branch, Lung classification, MI-GAN, Multi-domain translation},
	pages = {107200},
	file = {PDF:C\:\\Users\\jakob\\Zotero\\storage\\DT55LTR8\\Chen et al. - 2023 - Multi-domain medical image translation generation for lung image classification based on generative.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\WTDMEPQF\\S0169260722005818.html:text/html},
}

@article{ozkan_multi-domain_2024,
	title = {Multi-domain improves classification in out-of-distribution and data-limited scenarios for medical image analysis},
	volume = {14},
	copyright = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-73561-y},
	doi = {10.1038/s41598-024-73561-y},
	abstract = {Current machine learning methods for medical image analysis primarily focus on developing models tailored for their specific tasks, utilizing data within their target domain. These specialized models tend to be data-hungry and often exhibit limitations in generalizing to out-of-distribution samples. In this work, we show that employing models that incorporate multiple domains instead of specialized ones significantly alleviates the limitations observed in specialized models. We refer to this approach as multi-domain model and compare its performance to that of specialized models. For this, we introduce the incorporation of diverse medical image domains, including different imaging modalities like X-ray, MRI, CT, and ultrasound images, as well as various viewpoints such as axial, coronal, and sagittal views. Our findings underscore the superior generalization capabilities of multi-domain models, particularly in scenarios characterized by limited data availability and out-of-distribution, frequently encountered in healthcare applications. The integration of diverse data allows multi-domain models to utilize information across domains, enhancing the overall outcomes substantially. To illustrate, for organ recognition, multi-domain model can enhance accuracy by up to 8\% compared to conventional specialized models.},
	language = {en},
	number = {1},
	urldate = {2026-01-30},
	journal = {Scientific Reports},
	publisher = {Nature Publishing Group},
	author = {Ozkan, Ece and Boix, Xavier},
	month = oct,
	year = {2024},
	keywords = {Medical research, Biomedical engineering},
	pages = {24412},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\MX4J8DQR\\Ozkan und Boix - 2024 - Multi-domain improves classification in out-of-distribution and data-limited scenarios for medical i.pdf:application/pdf},
}

@article{armanious_medgan_2020,
	title = {{MedGAN}: {Medical} image translation using {GANs}},
	volume = {79},
	issn = {0895-6111},
	shorttitle = {{MedGAN}},
	url = {https://www.sciencedirect.com/science/article/pii/S0895611119300990},
	doi = {10.1016/j.compmedimag.2019.101684},
	abstract = {Image-to-image translation is considered a new frontier in the field of medical image analysis, with numerous potential applications. However, a large portion of recent approaches offers individualized solutions based on specialized task-specific architectures or require refinement through non-end-to-end training. In this paper, we propose a new framework, named MedGAN, for medical image-to-image translation which operates on the image level in an end-to-end manner. MedGAN builds upon recent advances in the field of generative adversarial networks (GANs) by merging the adversarial framework with a new combination of non-adversarial losses. We utilize a discriminator network as a trainable feature extractor which penalizes the discrepancy between the translated medical images and the desired modalities. Moreover, style-transfer losses are utilized to match the textures and fine-structures of the desired target images to the translated images. Additionally, we present a new generator architecture, titled CasNet, which enhances the sharpness of the translated medical outputs through progressive refinement via encoder-decoder pairs. Without any application-specific modifications, we apply MedGAN on three different tasks: PET-CT translation, correction of MR motion artefacts and PET image denoising. Perceptual analysis by radiologists and quantitative evaluations illustrate that the MedGAN outperforms other existing translation approaches.},
	urldate = {2026-01-30},
	journal = {Computerized Medical Imaging and Graphics},
	author = {Armanious, Karim and Jiang, Chenming and Fischer, Marc and Küstner, Thomas and Hepp, Tobias and Nikolaou, Konstantin and Gatidis, Sergios and Yang, Bin},
	month = jan,
	year = {2020},
	keywords = {Generative adversarial networks, Deep neural networks, Image translation, MR motion correction, PET attenuation correction},
	pages = {101684},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\SCI9CIV9\\Armanious et al. - 2020 - MedGAN Medical image translation using GANs.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\6V7GJAH9\\S0895611119300990.html:text/html},
}

@misc{falkiewicz_kolmogorov-smirnov_2024,
	title = {Kolmogorov-{Smirnov} {GAN}},
	url = {http://arxiv.org/abs/2406.19948},
	doi = {10.48550/arXiv.2406.19948},
	abstract = {We propose a novel deep generative model, the Kolmogorov-Smirnov Generative Adversarial Network (KSGAN). Unlike existing approaches, KSGAN formulates the learning process as a minimization of the Kolmogorov-Smirnov (KS) distance, generalized to handle multivariate distributions. This distance is calculated using the quantile function, which acts as the critic in the adversarial training process. We formally demonstrate that minimizing the KS distance leads to the trained approximate distribution aligning with the target distribution. We propose an efficient implementation and evaluate its effectiveness through experiments. The results show that KSGAN performs on par with existing adversarial methods, exhibiting stability during training, resistance to mode dropping and collapse, and tolerance to variations in hyperparameter settings. Additionally, we review the literature on the Generalized KS test and discuss the connections between KSGAN and existing adversarial generative models.},
	language = {en},
	urldate = {2026-02-02},
	publisher = {arXiv},
	author = {Falkiewicz, Maciej and Takeishi, Naoya and Kalousis, Alexandros},
	month = jun,
	year = {2024},
	note = {arXiv:2406.19948 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:C\:\\Users\\jakob\\Zotero\\storage\\UNWYMVG5\\Falkiewicz et al. - 2024 - Kolmogorov-Smirnov GAN.pdf:application/pdf},
}

@inproceedings{kim_adaptive_2024,
	title = {Adaptive {Latent} {Diffusion} {Model} for {3D} {Medical} {Image} to {Image} {Translation}: {Multi}-{Modal} {Magnetic} {Resonance} {Imaging} {Study}},
	shorttitle = {Adaptive {Latent} {Diffusion} {Model} for {3D} {Medical} {Image} to {Image} {Translation}},
	url = {https://openaccess.thecvf.com/content/WACV2024/html/Kim_Adaptive_Latent_Diffusion_Model_for_3D_Medical_Image_to_Image_WACV_2024_paper.html},
	language = {en},
	urldate = {2026-02-02},
	author = {Kim, Jonghun and Park, Hyunjin},
	year = {2024},
	pages = {7604--7613},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\HVB3V34H\\Kim und Park - 2024 - Adaptive Latent Diffusion Model for 3D Medical Image to Image Translation Multi-Modal Magnetic Reso.pdf:application/pdf},
}

@misc{sagers_augmenting_2023,
	title = {Augmenting medical image classifiers with synthetic data from latent diffusion models},
	url = {http://arxiv.org/abs/2308.12453},
	doi = {10.48550/arXiv.2308.12453},
	abstract = {While hundreds of artificial intelligence (AI) algorithms are now approved or cleared by the US Food and Drugs Administration (FDA)[1], many studies have shown inconsistent generalization or latent bias, particularly for underrepresented populations[2–4]. Some have proposed that generative AI[5, 6] could reduce the need for real data[7], but its utility in model development remains unclear. Skin disease serves as a useful case study in synthetic image generation due to the diversity of disease appearance, particularly across the protected attribute of skin tone. Here we show that latent diffusion models can scalably generate images of skin disease and that augmenting model training with these data improves performance in data-limited settings. These performance gains saturate at synthetic-to-real image ratios above 10:1 and are substantially smaller than the gains obtained from adding real images. As part of our analysis, we generate and analyze a new dataset of 458,920 synthetic images produced using several generation strategies. Our results suggest that synthetic data could serve as a force-multiplier for model development, but the collection of diverse real-world data remains the most important step to improve medical AI algorithms.},
	language = {en},
	urldate = {2026-02-02},
	publisher = {arXiv},
	author = {Sagers, Luke W. and Diao, James A. and Melas-Kyriazi, Luke and Groh, Matthew and Rajpurkar, Pranav and Adamson, Adewole S. and Rotemberg, Veronica and Daneshjou, Roxana and Manrai, Arjun K.},
	month = aug,
	year = {2023},
	note = {arXiv:2308.12453 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {PDF:C\:\\Users\\jakob\\Zotero\\storage\\FPGZQE55\\Sagers et al. - 2023 - Augmenting medical image classifiers with synthetic data from latent diffusion models.pdf:application/pdf},
}

@misc{binkowski_demystifying_2021,
	title = {Demystifying {MMD} {GANs}},
	url = {http://arxiv.org/abs/1801.01401},
	doi = {10.48550/arXiv.1801.01401},
	abstract = {We investigate the training and performance of generative adversarial networks using the Maximum Mean Discrepancy (MMD) as critic, termed MMD GANs. As our main theoretical contribution, we clarify the situation with bias in GAN loss functions raised by recent work: we show that gradient estimators used in the optimization process for both MMD GANs and Wasserstein GANs are unbiased, but learning a discriminator based on samples leads to biased gradients for the generator parameters. We also discuss the issue of kernel choice for the MMD critic, and characterize the kernel corresponding to the energy distance used for the Cramér GAN critic. Being an integral probability metric, the MMD beneﬁts from training strategies recently developed for Wasserstein GANs. In experiments, the MMD GAN is able to employ a smaller critic network than the Wasserstein GAN, resulting in a simpler and faster-training algorithm with matching performance. We also propose an improved measure of GAN convergence, the Kernel Inception Distance, and show how to use it to dynamically adapt learning rates during GAN training.},
	language = {en},
	urldate = {2026-02-02},
	publisher = {arXiv},
	author = {Bińkowski, Mikołaj and Sutherland, Danica J. and Arbel, Michael and Gretton, Arthur},
	month = jan,
	year = {2021},
	note = {arXiv:1801.01401 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {PDF:C\:\\Users\\jakob\\Zotero\\storage\\UEKIS8ND\\Bińkowski et al. - 2021 - Demystifying MMD GANs.pdf:application/pdf},
}

@article{sejdinovic_equivalence_2013,
	title = {Equivalence of {Distance}-{Based} and {Rkhs}-{Based} {Statistics} in {Hypothesis} {Testing}},
	volume = {41},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/23566550},
	abstract = {We provide a unifying framework linking two classes of statistics used in two-sample and independence testing: on the one hand, the energy distances and distance covariances from the statistics literature; on the other, maximum mean discrepancies (MMD), that is, distances between embeddings of distributions to reproducing kernel Hilbert spaces (RKHS), as established in machine learning. In the case where the energy distance is computed with a semimetric of negative type, a positive definite kernel, termed distance kernel, may be defined such that the MMD corresponds exactly to the energy distance. Conversely, for any positive definite kernel, we can interpret the MMD as energy distance with respect to some negative-type semimetric. This equivalence readily extends to distance covariance using kernels on the product space. We determine the class of probability distributions for which the test statistics are consistent against all alternatives. Finally, we investigate the performance of the family of distance kernels in two-sample and independence tests: we show in particular that the energy distance most commonly employed in statistics is just one member of a parametric family of kernels, and that other choices from this family can yield more powerful tests.},
	number = {5},
	urldate = {2026-02-02},
	journal = {The Annals of Statistics},
	publisher = {Institute of Mathematical Statistics},
	author = {Sejdinovic, Dino and Sriperumbudur, Bharath and Gretton, Arthur and Fukumizu, Kenji},
	year = {2013},
	pages = {2263--2291},
	file = {JSTOR Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\V2VHYR4U\\Sejdinovic et al. - 2013 - Equivalence of Distance-Based and Rkhs-Based Statistics in Hypothesis Testing.pdf:application/pdf},
}

@article{xu_attcl-gan_2026,
	title = {{AttCL}-{GAN}: {Attentional} contrastive learning-based generative adversarial network for modality completion of medical images},
	volume = {334},
	issn = {0950-7051},
	shorttitle = {{AttCL}-{GAN}},
	url = {https://www.sciencedirect.com/science/article/pii/S0950705125020556},
	doi = {10.1016/j.knosys.2025.115017},
	abstract = {Most multi-modal medical image analysis models require datasets to be modal-balanced. However, existing multi-modal medical datasets often suffer from modality missing problems due to practical factors. Therefore, in this study, we propose a novel generative model, AttCL-GAN, to remedy three issues encountered when applying the multi-domain model StarGAN v2 to medical images generation. Further by utilizing the proposed modality completion strategy, we achieve the data imputation under arbitrary modality missing scenarios. The advancements of AttCL-GAN are mainly threefold: Firstly, we integrate the idea of contrastive learning and attention mechanism to guide the network in learning domain-independent content during image translations using attention information from spatial and channel dimensions. Then, the attentional AdaIN generator is introduced into the network. By adding the refined features and original features together and feeding them into AdaIN, the generator’s ability to generate tissue details is enhanced. Finally, to alleviate the problem of high inter-modal similarity of generated images, we propose a style code diversity loss, which increases the diversity of images by enlarging the Euclidean distance between codes of different modalities in the latent space. Extensive experimental results on a real-world multi-modal brain MRI dataset show that (i) The proposed AttCL-GAN significantly outperforms the state-of-the-art GAN-based data augmentation methods in both the generation and segmentation tasks in terms of all metrics; (ii) The proposed three advancements are all effective and essential for AttCL-GAN to achieve the superior performances in both tasks.},
	urldate = {2026-02-02},
	journal = {Knowledge-Based Systems},
	author = {Xu, Zhenghua and Tang, Jiaqi and Yao, Dan and Wang, Zhenzhen and Lukasiewicz, Thomas},
	month = feb,
	year = {2026},
	keywords = {Generative adversarial networks, Attention mechanism, Contrastive learning, Modality completion of medical images, Multi-modal image segmentation},
	pages = {115017},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\Y876ZU2V\\Xu et al. - 2026 - AttCL-GAN Attentional contrastive learning-based generative adversarial network for modality comple.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\UUKNCJTR\\S0950705125020556.html:text/html},
}

@inproceedings{takemoto_multi-datasets_2025,
	title = {Multi-{Datasets} {Learning} for {CT} {Image} {Generation} {From} {MR} {Image} {Using} {StarGAN} v2},
	issn = {2693-0854},
	url = {https://ieeexplore.ieee.org/document/11274921/},
	doi = {10.1109/GCCE65946.2025.11274921},
	abstract = {Computed Tomography (CT) image is essential for the diagnosis and treatment planning but involves radiation exposure. Converting radiation-free Magnetic Resonance (MR) images into CT-like images can mitigate this risk. However, it is difficult to generate images with high-quality with small dataset. Therefore, we aim to improve the quality of the generated images by training on both the target dataset and auxiliary dataset using StarGAN v2, which learns multiple domain image transformations.},
	urldate = {2026-02-04},
	booktitle = {2025 {IEEE} 14th {Global} {Conference} on {Consumer} {Electronics} ({GCCE})},
	author = {Takemoto, Yuki and Iwamoto, Yutaro and Nonaka, Masahiro and Chen, Yen-Wei},
	month = sep,
	year = {2025},
	note = {ISSN: 2693-0854},
	keywords = {Image synthesis, Training, image generation, Computed tomography, Consumer electronics, CT image, Image transformation, Magnetic resonance, multiple domains, Planning, StarGAN v2},
	pages = {1173--1177},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\XWXYZHUG\\Takemoto et al. - 2025 - Multi-Datasets Learning for CT Image Generation From MR Image Using StarGAN v2.pdf:application/pdf},
}

@article{chen_unsupervised_2023,
	title = {Unsupervised image-to-image translation in multi-parametric {MRI} of bladder cancer},
	volume = {124},
	issn = {0952-1976},
	url = {https://www.sciencedirect.com/science/article/pii/S0952197623007315},
	doi = {10.1016/j.engappai.2023.106547},
	abstract = {Detection of muscular invasive bladder cancer (MIBC) is critical for surgical selection of bladder cancer (BCa) patients. Currently, multi-parameter magnetic resonance imaging (mp-MRI) is the predominant approach for identifying MIBC. However, mp-MRI is still insufficient due to the presence of noise and artifacts. Our research aims to synthesize images from the existing sequences of mp-MRI to substitute missing or low signal-to-noise ratio sequences through image-to-image (I2I) translation. Using mp-MRI images of 255 BCa patients collected in our department, we here propose a one-to-many unsupervised I2I translation network with region-wise semantic enhancement to synthesize virtual samples. We introduce an improved adaptive instance normalization module to support the generator for synthesizing multi-domain images. In addition, a branch for region-wise semantic segmentation helps the generator to enhance the quality of image translation for a specific region. A semantically consistent loss is applied to maintaining the consistency between the synthesized and the input images via region-wise semantic segmentation. Experiments on the BraTS and BCa datasets indicate that our I2I translation approach outperforms several state of the art methods. Additionally, we perform clinical feasibility tests using the synthesis images. The clinicians reach a consensus between the Vesical Imaging Reporting and Data System (VI-RADS) scoring results from the synthesized and the real mp-MRI images. In addition, after the BCa training set has been expanded using the proposed generation model, the accuracy of the BCa muscular invasion classification is improved from 77.78\% to 85.19\%.},
	urldate = {2026-02-04},
	journal = {Engineering Applications of Artificial Intelligence},
	author = {Chen, Zhiying and Cai, Lingkai and Chen, Chunxiao and Fu, Xue and Yang, Xiao and Yuan, Baorui and Lu, Qiang and Zhou, Huiyu},
	month = sep,
	year = {2023},
	keywords = {Bladder cancer, Image-to-image translation, Multi-parameter MRI, Semantic enhancement},
	pages = {106547},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\YLZA2Y9X\\Chen et al. - 2023 - Unsupervised image-to-image translation in multi-parametric MRI of bladder cancer.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\KVYBTQQX\\S0952197623007315.html:text/html},
}

@article{holmes_facial_2023,
	title = {Facial skin disease prediction using {StarGAN} v2 and transfer learning},
	volume = {17},
	issn = {1872-4981},
	url = {https://journals.sagepub.com/action/showAbstract},
	doi = {10.3233/IDT-228046},
	abstract = {Deep learning algorithms have become the most prominent methods for medical image analysis over the past years, leading to enhanced performances in various medical applications. In this paper, we focus on applying intelligent skin disease detection to face images, where the crucial challenge is the low availability of training data. To achieve high disease detection and classification success rates, we adapt the state-of-the-art StarGAN v2 network to augment images of faces and combine it with a transfer learning approach. The experimental results show that the classification accuracies of transfer learning models are in the range of 77.46–99.80\% when trained on datasets that are extended with StarGAN v2 augmented data.},
	language = {EN},
	number = {1},
	urldate = {2026-02-04},
	journal = {Intelligent Decision Technologies},
	publisher = {SAGE Publications},
	author = {Holmes, Kristen and Sharma, Poonam and Fernandes, Steven},
	month = feb,
	year = {2023},
	pages = {55--66},
	file = {SAGE PDF Full Text:C\:\\Users\\jakob\\Zotero\\storage\\K9KTN7WD\\Holmes et al. - 2023 - Facial skin disease prediction using StarGAN v2 and transfer learning.pdf:application/pdf},
}

@article{luo_target-guided_2024,
	title = {Target-{Guided} {Diffusion} {Models} for {Unpaired} {Cross}-{Modality} {Medical} {Image} {Translation}},
	volume = {28},
	issn = {2168-2208},
	url = {https://ieeexplore.ieee.org/abstract/document/10508481},
	doi = {10.1109/JBHI.2024.3393870},
	abstract = {In a clinical setting, the acquisition of certain medical image modality is often unavailable due to various considerations such as cost, radiation, etc. Therefore, unpaired cross-modality translation techniques, which involve training on the unpaired data and synthesizing the target modality with the guidance of the acquired source modality, are of great interest. Previous methods for synthesizing target medical images are to establish one-shot mapping through generative adversarial networks (GANs). As promising alternatives to GANs, diffusion models have recently received wide interests in generative tasks. In this paper, we propose a target-guided diffusion model (TGDM) for unpaired cross-modality medical image translation. For training, to encourage our diffusion model to learn more visual concepts, we adopted a perception prioritized weight scheme (P2W) to the training objectives. For sampling, a pre-trained classifier is adopted in the reverse process to relieve modality-specific remnants from source data. Experiments on both brain MRI-CT and prostate MRI-US datasets demonstrate that the proposed method achieves a visually realistic result that mimics a vivid anatomical section of the target organ. In addition, we have also conducted a subjective assessment based on the synthesized samples to further validate the clinical value of TGDM.},
	number = {7},
	urldate = {2026-02-04},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Luo, Yimin and Yang, Qinyu and Liu, Ziyi and Shi, Zenglin and Huang, Weimin and Zheng, Guoyan and Cheng, Jun},
	month = jul,
	year = {2024},
	keywords = {Biomedical imaging, Magnetic resonance imaging, medical image synthesis, Training, Task analysis, Data models, Computed tomography, Bioinformatics, cross-modality translation, diffusion model, guidance sampling, Unpaired learning},
	pages = {4062--4071},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\VED9PCU3\\Luo et al. - 2024 - Target-Guided Diffusion Models for Unpaired Cross-Modality Medical Image Translation.pdf:application/pdf},
}

@article{li_his-mmdm_2024,
	title = {His-{MMDM}: {Multi}-{Domain} and {Multi}-{Omics} {Translation} of {Histopathological} {Images} with {Diffusion} {Models}},
	volume = {n/a},
	copyright = {© 2026 The Author(s). Advanced Science published by Wiley-VCH GmbH},
	issn = {2198-3844},
	shorttitle = {His-{MMDM}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/advs.202518066},
	doi = {10.1002/advs.202518066},
	abstract = {Generative AI (GenAI) has advanced computational pathology through various image translation models. These models synthesize histopathological images from existing ones, facilitating tasks such as color normalization and virtual staining. Current models, while effective, are mostly dedicated to specific source-target domain pairs and lack scalability for multi-domain translations. Here, we introduce His-MMDM, a diffusion model-based framework enabling multi-domain and multi-omics histopathological image translation. His-MMDM is not only effective in performing existing tasks such as transforming cryosectioned images to FFPE ones and virtual immunohistochemical (IHC) staining but can also facilitate knowledge transfer between different tumor types and between primary and metastatic tumors. Additionally, it performs genomics- and/or transcriptomics-guided editing of histopathological images, illustrating the impact of driver mutations and oncogenic pathway alterations on tissue histopathology and educating pathologists to recognize them. These versatile capabilities position His-MMDM as a versatile tool in the GenAI toolkit for future pathologists.},
	language = {en},
	number = {n/a},
	urldate = {2026-02-04},
	journal = {Advanced Science},
	author = {Li, Zhongxiao and Su, Tianqi and Zhang, Bin and Han, Wenkai and Zhang, Sibin and Sun, Guiyin and Cong, Yuwei and Chen, Xin and Qi, Jiping and Wang, Yujie and Zhao, Shiguang and Meng, Hongxue and Liang, Peng and Gao, Xin},
	year = {2024},
	note = {\_eprint: https://advanced.onlinelibrary.wiley.com/doi/pdf/10.1002/advs.202518066},
	keywords = {diffusion models, deep learning, generative artificial intelligence, histopathological image analysis},
	pages = {e18066},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\84UHB26H\\Li et al. - His-MMDM Multi-Domain and Multi-Omics Translation of Histopathological Images with Diffusion Models.pdf:application/pdf;Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\D4AIMALM\\advs.html:text/html},
}

@article{jiang_fast-ddpm_2025,
	title = {Fast-{DDPM}: {Fast} {Denoising} {Diffusion} {Probabilistic} {Models} for {Medical} {Image}-to-{Image} {Generation}},
	volume = {29},
	issn = {2168-2208},
	shorttitle = {Fast-{DDPM}},
	url = {https://ieeexplore.ieee.org/abstract/document/10979336},
	doi = {10.1109/JBHI.2025.3565183},
	abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved unprecedented success in computer vision. However, they remain underutilized in medical imaging, a field crucial for disease diagnosis and treatment planning. This is primarily due to the high computational cost associated with the use of large number of time steps (e.g., 1,000) in diffusion processes. Training a diffusion model on medical images typically takes days to weeks, while sampling each image volume takes minutes to hours. To address this challenge, we introduce Fast-DDPM, a simple yet effective approach capable of simultaneously improving training speed, sampling speed, and generation quality. Unlike DDPM, which trains the image denoiser across 1,000 time steps, Fast-DDPM trains and samples using only 10 time steps. The key to our method lies in aligning the training and sampling procedures to optimize time-step utilization. Specifically, we introduced two efficient noise schedulers with 10 time steps: one with uniform time step sampling and another with non-uniform sampling. We evaluated Fast-DDPM across three medical image-to-image generation tasks: multi-image super-resolution, image denoising, and image-to-image translation. Fast-DDPM outperformed DDPM and current state-of-the-art methods based on convolutional networks and generative adversarial networks in all tasks. Additionally, Fast-DDPM reduced the training time to 0.2× and the sampling time to 0.01× compared to DDPM.},
	number = {10},
	urldate = {2026-02-04},
	journal = {IEEE Journal of Biomedical and Health Informatics},
	author = {Jiang, Hongxu and Imran, Muhammad and Zhang, Teng and Zhou, Yuyin and Liang, Muxuan and Gong, Kuang and Shao, Wei},
	month = oct,
	year = {2025},
	keywords = {Medical diagnostic imaging, Diffusion models, Deep learning, deep learning, Diffusion processes, Noise reduction, Conditional diffusion models, fast-DDPM, Gaussian noise, image-to-image generation, Quantization (signal), Stochastic processes, Three-dimensional displays},
	pages = {7326--7335},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\2T37AF3F\\Jiang et al. - 2025 - Fast-DDPM Fast Denoising Diffusion Probabilistic Models for Medical Image-to-Image Generation.pdf:application/pdf},
}

@article{konz_frechet_2026,
	title = {Fréchet {Radiomic} {Distance} ({FRD}): {A} {Versatile} {Metric} for {Comparing} {Medical} {Imaging} {Datasets}},
	volume = {110},
	issn = {13618415},
	shorttitle = {Fréchet {Radiomic} {Distance} ({FRD})},
	url = {http://arxiv.org/abs/2412.01496},
	doi = {10.1016/j.media.2026.103943},
	abstract = {Determining whether two sets of images belong to the same or different distributions or domains is a crucial task in modern medical image analysis and deep learning; for example, to evaluate the output quality of image generative models. Currently, metrics used for this task either rely on the (potentially biased) choice of some downstream task, such as segmentation, or adopt task-independent perceptual metrics (e.g., Fréchet Inception Distance/FID) from natural imaging, which we show insufficiently capture anatomical features. To this end, we introduce a new perceptual metric tailored for medical images, FRD (Fréchet Radiomic Distance), which utilizes standardized, clinically meaningful, and interpretable image features. We show that FRD is superior to other image distribution metrics for a range of medical imaging applications, including out-of-domain (OOD) detection, the evaluation of image-to-image translation (by correlating more with downstream task performance as well as anatomical consistency and realism), and the evaluation of unconditional image generation. Moreover, FRD offers additional benefits such as stability and computational efficiency at low sample sizes, sensitivity to image corruptions and adversarial attacks, feature interpretability, and correlation with radiologist-perceived image quality. Additionally, we address key gaps in the literature by presenting an extensive framework for the multifaceted evaluation of image similarity metrics in medical imaging -- including the first large-scale comparative study of generative models for medical image translation -- and release an accessible codebase to facilitate future research. Our results are supported by thorough experiments spanning a variety of datasets, modalities, and downstream tasks, highlighting the broad potential of FRD for medical image analysis.},
	language = {en},
	urldate = {2026-02-06},
	journal = {Medical Image Analysis},
	author = {Konz, Nicholas and Osuala, Richard and Verma, Preeti and Chen, Yuwen and Gu, Hanxue and Dong, Haoyu and Chen, Yaqian and Marshall, Andrew and Garrucho, Lidia and Kushibar, Kaisar and Lang, Daniel M. and Kim, Gene S. and Grimm, Lars J. and Lewin, John M. and Duncan, James S. and Schnabel, Julia A. and Diaz, Oliver and Lekadir, Karim and Mazurowski, Maciej A.},
	month = may,
	year = {2026},
	note = {arXiv:2412.01496 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
	pages = {103943},
	file = {PDF:C\:\\Users\\jakob\\Zotero\\storage\\6LFHZ5P5\\Konz et al. - 2026 - Fréchet Radiomic Distance (FRD) A Versatile Metric for Comparing Medical Imaging Datasets.pdf:application/pdf},
}

@misc{zou_cyclediff_2026,
	title = {{CycleDiff}: {Cycle} {Diffusion} {Models} for {Unpaired} {Image}-to-image {Translation}},
	shorttitle = {{CycleDiff}},
	url = {http://arxiv.org/abs/2508.06625},
	doi = {10.48550/arXiv.2508.06625},
	abstract = {We introduce a diffusion-based cross-domain image translator in the absence of paired training data. Unlike GANbased methods, our approach integrates diffusion models to learn the image translation process, allowing for more coverable modeling of the data distribution and performance improvement of the cross-domain translation. However, incorporating the translation process within the diffusion process is still challenging since the two processes are not aligned exactly, i.e., the diffusion process is applied to the noisy signal while the translation process is conducted on the clean signal. As a result, recent diffusionbased studies employ separate training or shallow integration to learn the two processes, yet this may cause the local minimal of the translation optimization, constraining the effectiveness of diffusion models. To address the problem, we propose a novel joint learning framework that aligns the diffusion and the translation process, thereby improving the global optimality. Specifically, we propose to extract the image components with diffusion models to represent the clean signal and employ the translation process with the image components, enabling an endto-end joint learning manner. On the other hand, we introduce a time-dependent translation network to learn the complex translation mapping, resulting in effective translation learning and significant performance improvement. Benefiting from the design of joint learning, our method enables global optimization of both processes, enhancing the optimality and achieving improved fidelity and structural consistency. We have conducted extensive experiments on RGB↔RGB and diverse cross-modality translation tasks including RGB↔Edge, RGB↔Semantics and RGB↔Depth, showcasing better generative performances than the state of the arts. Especially, our method achieves the best FID score in widely-adopted tasks and outperforms the second-best method with an improved FID of 19.61 and 19.67 on Dog→Cat and Dog→Wild respectively.},
	language = {en},
	urldate = {2026-02-11},
	publisher = {arXiv},
	author = {Zou, Shilong and Huang, Yuhang and Yi, Renjiao and Zhu, Chenyang and Xu, Kai},
	month = jan,
	year = {2026},
	note = {arXiv:2508.06625 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {PDF:C\:\\Users\\jakob\\Zotero\\storage\\364JY2NG\\Zou et al. - 2026 - CycleDiff Cycle Diffusion Models for Unpaired Image-to-image Translation.pdf:application/pdf},
}

@misc{wu_unifying_2022,
	title = {Unifying {Diffusion} {Models}' {Latent} {Space}, with {Applications} to {CycleDiffusion} and {Guidance}},
	url = {http://arxiv.org/abs/2210.05559},
	doi = {10.48550/arXiv.2210.05559},
	abstract = {Diffusion models have achieved unprecedented performance in generative modeling. The commonly-adopted formulation of the latent code of diffusion models is a sequence of gradually denoised samples, as opposed to the simpler (e.g., Gaussian) latent space of GANs, VAEs, and normalizing flows. This paper provides an alternative, Gaussian formulation of the latent space of various diffusion models, as well as an invertible DPM-Encoder that maps images into the latent space. While our formulation is purely based on the definition of diffusion models, we demonstrate several intriguing consequences. (1) Empirically, we observe that a common latent space emerges from two diffusion models trained independently on related domains. In light of this finding, we propose CycleDiffusion, which uses DPM-Encoder for unpaired image-to-image translation. Furthermore, applying CycleDiffusion to text-to-image diffusion models, we show that large-scale text-to-image diffusion models can be used as zero-shot image-to-image editors. (2) One can guide pre-trained diffusion models and GANs by controlling the latent codes in a unified, plug-and-play formulation based on energy-based models. Using the CLIP model and a face recognition model as guidance, we demonstrate that diffusion models have better coverage of low-density sub-populations and individuals than GANs. The code is publicly available at https://github.com/ChenWu98/cycle-diffusion.},
	urldate = {2026-02-11},
	publisher = {arXiv},
	author = {Wu, Chen Henry and Torre, Fernando De la},
	month = dec,
	year = {2022},
	note = {arXiv:2210.05559 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Graphics},
	file = {Preprint PDF:C\:\\Users\\jakob\\Zotero\\storage\\GV59EC5U\\Wu und Torre - 2022 - Unifying Diffusion Models' Latent Space, with Applications to CycleDiffusion and Guidance.pdf:application/pdf;Snapshot:C\:\\Users\\jakob\\Zotero\\storage\\LN46ZU9C\\2210.html:text/html},
}

@article{schilcher_bisphosphonate_2011,
	title = {Bisphosphonate {Use} and {Atypical} {Fractures} of the {Femoral} {Shaft}},
	volume = {364},
	issn = {0028-4793},
	url = {https://www.nejm.org/doi/full/10.1056/NEJMoa1010650},
	doi = {10.1056/NEJMoa1010650},
	abstract = {Radiographs of Swedish women with atypical subtrochanteric fractures were compared with radiographs of women with ordinary subtrochanteric or shaft fractures. The increase in absolute risk was 5 cases per 10,000 patient-years of bisphosphonate use. Bisphosphonates reduce the overall risk of fracture among patients with osteoporosis, with a long-lasting beneficial effect.1 However, since bisphosphonates reduce bone remodeling, they might “freeze” the skeleton, allowing accumulation of microcracks over time, leading to fatigue fractures (also called stress fractures).2 Fatigue fractures are well known in mechanical engineering, and they occur with age and overload in many materials. They result from the slow propagation of cracks, resulting in a peculiar appearance. In bone, their appearance is characterized by a straight fracture line running perpendicularly to tractional forces. Such fractures are not uncommon in athletes, and if they are not . . .},
	number = {18},
	urldate = {2026-02-14},
	journal = {New England Journal of Medicine},
	publisher = {Massachusetts Medical Society},
	author = {Schilcher, Jörg and Michaëlsson, Karl and Aspenberg, Per},
	month = may,
	year = {2011},
	note = {\_eprint: https://www.nejm.org/doi/pdf/10.1056/NEJMoa1010650},
	pages = {1728--1737},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\2PNMBAW6\\Schilcher et al. - 2011 - Bisphosphonate Use and Atypical Fractures of the Femoral Shaft.pdf:application/pdf},
}

@article{ghavidel_machine_2025,
	title = {Machine learning ({ML}) techniques to predict breast cancer in imbalanced datasets: a systematic review},
	volume = {19},
	issn = {1932-2267},
	shorttitle = {Machine learning ({ML}) techniques to predict breast cancer in imbalanced datasets},
	url = {https://doi.org/10.1007/s11764-023-01465-3},
	doi = {10.1007/s11764-023-01465-3},
	abstract = {Knowledge discovery in databases (KDD) is crucial in analyzing data to extract valuable insights. In medical outcome prediction, KDD is increasingly applied, particularly in diseases with high incidence, mortality, and costs, like cancer. ML techniques can develop more accurate predictive models for cancer patients’ clinical outcomes, aiding informed healthcare decision-making. However, cancer prediction modeling faces challenges because of the unbalanced nature of the datasets, where there is a small minority category of patients with a cancer diagnosis compared to a majority category of cancer-free patients. Imbalanced datasets pose statistical hurdles like bias and overfitting when developing accurate prediction models. This systematic review focuses on breast cancer prediction articles published from 2008 to 2023. The objective is to examine ML methods used in three critical steps of KDD: preprocessing, data mining, and interpretation which address the imbalanced data problem in breast cancer prediction. This work synthesizes prior research in ML methods for breast cancer prediction. The findings help identify effective preprocessing strategies, including balancing and feature selection methods, robust predictive models, and evaluation metrics of those models. The study aims to inform healthcare providers and researchers about effective techniques for accurate breast cancer prediction.},
	language = {en},
	number = {1},
	urldate = {2026-02-16},
	journal = {Journal of Cancer Survivorship},
	author = {Ghavidel, Arman and Pazos, Pilar},
	month = feb,
	year = {2025},
	keywords = {Cancer, Classification, Feature selection, Imbalanced data, Resampling},
	pages = {270--294},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\UVK76XD2\\Ghavidel und Pazos - 2025 - Machine learning (ML) techniques to predict breast cancer in imbalanced datasets a systematic revie.pdf:application/pdf},
}

@article{bogl_increased_2020,
	title = {Increased rate of reoperation in atypical femoral fractures is related to patient characteristics and not fracture type. {A} nationwide cohort study},
	volume = {31},
	issn = {1433-2965},
	url = {https://doi.org/10.1007/s00198-019-05249-3},
	doi = {10.1007/s00198-019-05249-3},
	abstract = {Atypical femoral fractures are burdened with a high rate of reoperation. In our nationwide analysis, the increased rate of reoperation was related to patient background characteristics, such as age and health status, rather than fracture type.},
	language = {en},
	number = {5},
	urldate = {2026-02-16},
	journal = {Osteoporosis International},
	author = {Bögl, H.P. and Michaëlsson, K. and Zdolsek, G. and Höijer, J. and Schilcher, J.},
	month = may,
	year = {2020},
	keywords = {Atypical femoral fracture, Bisphosphonates, Complications, Osteoporosis, Reoperation risk},
	pages = {951--959},
	file = {Full Text PDF:C\:\\Users\\jakob\\Zotero\\storage\\483IZXHH\\Bögl et al. - 2020 - Increased rate of reoperation in atypical femoral fractures is related to patient characteristics an.pdf:application/pdf},
}
